{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dba6a731",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Independent Tests for Bounding Box Validation on YOLO Model\n",
    "\n",
    "Using the Thermal Model for testing the Bounding Box Validation (BBV) Pipeline & Model on a live YOLO model, using manually counted clips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41e50ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'Python' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "# Run bash to install tkinter (requires sudo privileges and apt)\n",
    "!sudo apt-get update -y && sudo apt-get install -y python3-tk\n",
    "# Ensure that all dependencies are installed\n",
    "# Install 'uv' (if available), create a virtual environment, activate it in this shell,\n",
    "# then install requirements into that venv.\n",
    "%pip install uv || true\n",
    "%python3 -m venv .venv\n",
    "%. .venv/bin/activate && python -m pip install --upgrade pip setuptools wheel && python -m pip install uv && python -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "639cb6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n"
     ]
    }
   ],
   "source": [
    "''' Dependencies '''\n",
    "\n",
    "# Core libraries\n",
    "import cv2\n",
    "import supervision as sv\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from utils.thermal_frame_to_temp import result_to_temp_frame\n",
    "from utils.group_bounding_boxes import group_and_merge_bounding_boxes\n",
    "from validate_bounding_box import get_box_count\n",
    "from tkinter.filedialog import askopenfilename, askdirectory\n",
    "import joblib\n",
    "from collections import Counter\n",
    "\n",
    "# SVM and model training\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Synthesizing Data\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# ANN\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Better exception handling and helpers\n",
    "import traceback\n",
    "import pprint\n",
    "import datetime\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cfd629",
   "metadata": {},
   "source": [
    "## Deriving Ground Truth Video & Labeling\n",
    "\n",
    "Given a clip with a known count, save the known count for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a03da080",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRUE_COUNT = 75  # True count for the given clip\n",
    "# FILE_NAME:str = \"data/Brennen's-Thermal-Video/Top-Belt(Iron)-01-Testing-TRUE_COUNT-98.mp4\" # Global variable to store the selected file name if applicable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb15db2",
   "metadata": {},
   "source": [
    "## Pulling in the saved model & declaring pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7ffff57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  display: none;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  overflow: visible;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".estimator-table summary {\n",
       "    padding: .5rem;\n",
       "    font-family: monospace;\n",
       "    cursor: pointer;\n",
       "}\n",
       "\n",
       ".estimator-table details[open] {\n",
       "    padding-left: 0.1rem;\n",
       "    padding-right: 0.1rem;\n",
       "    padding-bottom: 0.3rem;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table {\n",
       "    margin-left: auto !important;\n",
       "    margin-right: auto !important;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(odd) {\n",
       "    background-color: #fff;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(even) {\n",
       "    background-color: #f6f6f6;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:hover {\n",
       "    background-color: #e0e0e0;\n",
       "}\n",
       "\n",
       ".estimator-table table td {\n",
       "    border: 1px solid rgba(106, 105, 104, 0.232);\n",
       "}\n",
       "\n",
       ".user-set td {\n",
       "    color:rgb(255, 94, 0);\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td.value pre {\n",
       "    color:rgb(255, 94, 0) !important;\n",
       "    background-color: transparent !important;\n",
       "}\n",
       "\n",
       ".default td {\n",
       "    color: black;\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td i,\n",
       ".default td i {\n",
       "    color: black;\n",
       "}\n",
       "\n",
       ".copy-paste-icon {\n",
       "    background-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA0NDggNTEyIj48IS0tIUZvbnQgQXdlc29tZSBGcmVlIDYuNy4yIGJ5IEBmb250YXdlc29tZSAtIGh0dHBzOi8vZm9udGF3ZXNvbWUuY29tIExpY2Vuc2UgLSBodHRwczovL2ZvbnRhd2Vzb21lLmNvbS9saWNlbnNlL2ZyZWUgQ29weXJpZ2h0IDIwMjUgRm9udGljb25zLCBJbmMuLS0+PHBhdGggZD0iTTIwOCAwTDMzMi4xIDBjMTIuNyAwIDI0LjkgNS4xIDMzLjkgMTQuMWw2Ny45IDY3LjljOSA5IDE0LjEgMjEuMiAxNC4xIDMzLjlMNDQ4IDMzNmMwIDI2LjUtMjEuNSA0OC00OCA0OGwtMTkyIDBjLTI2LjUgMC00OC0yMS41LTQ4LTQ4bDAtMjg4YzAtMjYuNSAyMS41LTQ4IDQ4LTQ4ek00OCAxMjhsODAgMCAwIDY0LTY0IDAgMCAyNTYgMTkyIDAgMC0zMiA2NCAwIDAgNDhjMCAyNi41LTIxLjUgNDgtNDggNDhMNDggNTEyYy0yNi41IDAtNDgtMjEuNS00OC00OEwwIDE3NmMwLTI2LjUgMjEuNS00OCA0OC00OHoiLz48L3N2Zz4=);\n",
       "    background-repeat: no-repeat;\n",
       "    background-size: 14px 14px;\n",
       "    background-position: 0;\n",
       "    display: inline-block;\n",
       "    width: 14px;\n",
       "    height: 14px;\n",
       "    cursor: pointer;\n",
       "}\n",
       "</style><body><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;classifier&#x27;,\n",
       "                 RandomForestClassifier(max_depth=10, max_features=0.8,\n",
       "                                        min_samples_split=4))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>Pipeline</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.7/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('steps',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">steps&nbsp;</td>\n",
       "            <td class=\"value\">[(&#x27;scaler&#x27;, ...), (&#x27;classifier&#x27;, ...)]</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('transform_input',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">transform_input&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('memory',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">memory&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('verbose',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">verbose&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>StandardScaler</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.7/modules/generated/sklearn.preprocessing.StandardScaler.html\">?<span>Documentation for StandardScaler</span></a></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"scaler__\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('copy',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">copy&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('with_mean',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">with_mean&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('with_std',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">with_std&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>RandomForestClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.7/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"classifier__\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('n_estimators',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">n_estimators&nbsp;</td>\n",
       "            <td class=\"value\">100</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('criterion',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">criterion&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;gini&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_depth',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">max_depth&nbsp;</td>\n",
       "            <td class=\"value\">10</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('min_samples_split',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">min_samples_split&nbsp;</td>\n",
       "            <td class=\"value\">4</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('min_samples_leaf',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">min_samples_leaf&nbsp;</td>\n",
       "            <td class=\"value\">1</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('min_weight_fraction_leaf',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">min_weight_fraction_leaf&nbsp;</td>\n",
       "            <td class=\"value\">0.0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_features',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">max_features&nbsp;</td>\n",
       "            <td class=\"value\">0.8</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_leaf_nodes',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">max_leaf_nodes&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('min_impurity_decrease',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">min_impurity_decrease&nbsp;</td>\n",
       "            <td class=\"value\">0.0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('bootstrap',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">bootstrap&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('oob_score',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">oob_score&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('n_jobs',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">n_jobs&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('random_state',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">random_state&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('verbose',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">verbose&nbsp;</td>\n",
       "            <td class=\"value\">0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('warm_start',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">warm_start&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('class_weight',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">class_weight&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('ccp_alpha',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">ccp_alpha&nbsp;</td>\n",
       "            <td class=\"value\">0.0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_samples',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">max_samples&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('monotonic_cst',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">monotonic_cst&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div></div></div><script>function copyToClipboard(text, element) {\n",
       "    // Get the parameter prefix from the closest toggleable content\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;\n",
       "\n",
       "    const originalStyle = element.style;\n",
       "    const computedStyle = window.getComputedStyle(element);\n",
       "    const originalWidth = computedStyle.width;\n",
       "    const originalHTML = element.innerHTML.replace('Copied!', '');\n",
       "\n",
       "    navigator.clipboard.writeText(fullParamName)\n",
       "        .then(() => {\n",
       "            element.style.width = originalWidth;\n",
       "            element.style.color = 'green';\n",
       "            element.innerHTML = \"Copied!\";\n",
       "\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        })\n",
       "        .catch(err => {\n",
       "            console.error('Failed to copy:', err);\n",
       "            element.style.color = 'red';\n",
       "            element.innerHTML = \"Failed!\";\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        });\n",
       "    return false;\n",
       "}\n",
       "\n",
       "document.querySelectorAll('.fa-regular.fa-copy').forEach(function(element) {\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const paramName = element.parentElement.nextElementSibling.textContent.trim();\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;\n",
       "\n",
       "    element.setAttribute('title', fullParamName);\n",
       "});\n",
       "</script></body>"
      ],
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('classifier',\n",
       "                 RandomForestClassifier(max_depth=10, max_features=0.8,\n",
       "                                        min_samples_split=4))])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BBV_MODEL = joblib.load('./models/thermal_chick_counting_rf_model_fit.pkl')  # Load the pre-trained BBV model\n",
    "BBV_STANDARD_SCALER = joblib.load('./models/thermal_chick_counting_rf_scaler.pkl')  # Load the pre-fitted StandardScaler\n",
    "YOLO_MODEL = YOLO('./models/new_iron.pt')  # Load the pre-trained YOLO model\n",
    "\n",
    "# Declare the pipeline\n",
    "BBV_PIPELINE = Pipeline([\n",
    "    ('scaler', BBV_STANDARD_SCALER),\n",
    "    ('classifier', BBV_MODEL)\n",
    "])\n",
    "\n",
    "# Validate that the model properly loaded\n",
    "BBV_PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b40b613",
   "metadata": {},
   "source": [
    "## Run the YOLO Model with Thermal Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7740794",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def get_line_from_video_frame(frame):\n",
    "    frame_height, frame_width = frame.shape[:2]\n",
    "    # Draw a horizontal line across the middle of the frame\n",
    "    line_start = (frame_width, frame_height // 2)\n",
    "    line_end = (0, frame_height // 2)\n",
    "    return [line_start, line_end]\n",
    "\n",
    "def chick_counting(video_path, output_path, line_points, verbose = False):\n",
    "\n",
    "    # Grab a sample frame so we know video size\n",
    "    generator = sv.get_video_frames_generator(video_path)\n",
    "    frame = next(generator)\n",
    "\n",
    "    # Set up video writer with same FPS/size as input\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    cap.release()\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (frame.shape[1], frame.shape[0]))\n",
    "    if not out.isOpened():\n",
    "        print(\"Error: Could not open video writer\")\n",
    "        return\n",
    "\n",
    "    # Init tracker and helpers\n",
    "    byte_tracker = sv.ByteTrack()\n",
    "    trace_annotator = sv.TraceAnnotator(thickness=4, trace_length=50)\n",
    "\n",
    "    # Create the counting line\n",
    "    line_zone = sv.LineZone(start=sv.Point(*line_points[0]), end=sv.Point(*line_points[1]))\n",
    "\n",
    "    # Load custom YOLO model\n",
    "    model = YOLO_MODEL\n",
    "    \n",
    "    # Annotators for boxes + labels\n",
    "    BOUNDING_BOX_ANNOTATOR = sv.BoxAnnotator(thickness=2, color=sv.Color(0, 255, 0))\n",
    "    LABEL_ANNOTATOR = sv.LabelAnnotator(text_scale=1)\n",
    "\n",
    "    # Counters\n",
    "    frame_count = 0\n",
    "    total_count = 0\n",
    "    total_count_bbv = 0\n",
    "    all_counted_ids = set()  # keep track of already-counted trackers\n",
    "    all_counted_ids_bbv = set()  # Seperate list for the bounding box validation\n",
    "    \n",
    "    # Constants to hold high and low thermal temperatures for denormalization\n",
    "    prev_hi = None\n",
    "    prev_lo = None\n",
    "\n",
    "    try:\n",
    "        generator = sv.get_video_frames_generator(video_path)\n",
    "\n",
    "        for frame in generator:\n",
    "            frame_count += 1\n",
    "            if verbose:\n",
    "                print(f\"Processing frame {frame_count}\")\n",
    "\n",
    "            # Run YOLO on frame\n",
    "            results = model(frame)[0]\n",
    "            \n",
    "            # Get the frame image as denormalized numpy array\n",
    "            try:\n",
    "                temp_arr, prev_hi, prev_lo = result_to_temp_frame(\n",
    "                    results,\n",
    "                    frame_idx = frame_count,\n",
    "                    prev_hi_val = prev_hi,\n",
    "                    prev_lo_val = prev_lo\n",
    "                )\n",
    "            except Exception as e:\n",
    "                if verbose:\n",
    "                    print(f\"Warning: Could not convert frame {frame_count} to temperature array. Skipping BBV for this frame.\")\n",
    "                if prev_hi is not None and prev_lo is not None:\n",
    "                    temp_arr = np.zeros_like(frame[..., 0], dtype=np.float32)  # reuse last temp_arr shape\n",
    "                else:\n",
    "                    temp_arr = None\n",
    "\n",
    "            # Convert results to supervision Detections\n",
    "            detections = sv.Detections.from_ultralytics(results)\n",
    "\n",
    "            # Sensitivity for declaring a box as \"nested\" (e.g. 0.9 means inner must have at least 90% of its area inside outer)\n",
    "            NESTED_THRESHOLD = 0.9  \n",
    "\n",
    "            # Get indicies of all boxes\n",
    "            contained_indices = set()\n",
    "            boxes = detections.xyxy\n",
    "\n",
    "            for i, outer in enumerate(boxes):\n",
    "                x1o, y1o, x2o, y2o = outer\n",
    "                outer_area = max(0, (x2o - x1o)) * max(0, (y2o - y1o))\n",
    "\n",
    "                for j, inner in enumerate(boxes):\n",
    "                    if i == j:\n",
    "                        continue\n",
    "                    x1i, y1i, x2i, y2i = inner\n",
    "                    inner_area = max(0, (x2i - x1i)) * max(0, (y2i - y1i))\n",
    "\n",
    "                    # Intersection box\n",
    "                    inter_x1 = max(x1o, x1i)\n",
    "                    inter_y1 = max(y1o, y1i)\n",
    "                    inter_x2 = min(x2o, x2i)\n",
    "                    inter_y2 = min(y2o, y2i)\n",
    "\n",
    "                    inter_area = max(0, inter_x2 - inter_x1) * max(0, inter_y2 - inter_y1)\n",
    "\n",
    "                    # Ratio of inner covered by outer\n",
    "                    if inner_area > 0 and (inter_area / inner_area) >= NESTED_THRESHOLD:\n",
    "                        contained_indices.add(j)\n",
    "\n",
    "\n",
    "            # Update tracker with detections\n",
    "            detections = byte_tracker.update_with_detections(detections)\n",
    "            if verbose:\n",
    "                print(\"Tracker IDs this frame:\", detections.tracker_id)\n",
    "\n",
    "            # See if any trackers crossed the line\n",
    "            crossed_in_flags, crossed_out_flags = line_zone.trigger(detections)\n",
    "\n",
    "            # Only count new IDs that cross \"in\"\n",
    "            for i, crossed in enumerate(crossed_in_flags):\n",
    "                if crossed:\n",
    "                    tracker_id = detections.tracker_id[i]\n",
    "                    \n",
    "                    if tracker_id is None: continue  # Skip if no tracker ID\n",
    "                    \n",
    "                    # YOLO tracker\n",
    "                    if tracker_id not in all_counted_ids:\n",
    "                        total_count += 1\n",
    "                        all_counted_ids.add(tracker_id)\n",
    "                        if verbose:\n",
    "                            print(f\"New Chick crossed the line! ID {tracker_id}, Total count: {total_count}\")\n",
    "                    \n",
    "                    # Bounding Box Validation tracker\n",
    "                    if tracker_id not in all_counted_ids_bbv:\n",
    "                        # Get the merged bounding boxes for overlaps in this frame\n",
    "                        group_and_merge_bounding_boxes_result = group_and_merge_bounding_boxes(\n",
    "                            xyxy = detections.xyxy, \n",
    "                            tracker_ids = detections.tracker_id.tolist(), \n",
    "                            target_tracker_id = tracker_id,\n",
    "                            iou_thresh = 0.10,  # Low threshold to catch even slight overlaps\n",
    "                        )\n",
    "                        if group_and_merge_bounding_boxes_result is None: continue  # Skip if no valid group found\n",
    "                        merged_box_group, grouped_tracker_ids = group_and_merge_bounding_boxes_result\n",
    "                        all_counted_ids_bbv.update(grouped_tracker_ids)  # Add all grouped IDs to counted list\n",
    "                        # Using the merged group, add to the total BBV count\n",
    "                        total_count_bbv += get_box_count(\n",
    "                            pipeline=BBV_PIPELINE,\n",
    "                            temperature_frame=temp_arr,\n",
    "                            box=merged_box_group\n",
    "                        )\n",
    "                        if verbose:\n",
    "                            print(f\"New BBV Chick(s) crossed the line! Grouped IDs {grouped_tracker_ids}, BBV Total count: {total_count_bbv}\")\n",
    "\n",
    "            # Assign labels + colors depending on nesting\n",
    "            labels = []\n",
    "            colors = []\n",
    "            for i, tracker_id in enumerate(detections.tracker_id):\n",
    "                if i in contained_indices:\n",
    "                    labels.append(f\"#{tracker_id} nested\")\n",
    "                    colors.append(sv.Color.RED)\n",
    "                else:\n",
    "                    labels.append(f\"#{tracker_id} chick\")\n",
    "                    colors.append(sv.Color.GREEN)\n",
    "\n",
    "            # Draw tracker trails\n",
    "            annotated_frame = trace_annotator.annotate(scene=frame.copy(), detections=detections)\n",
    "\n",
    "            # Draw bounding boxes manually with chosen colors\n",
    "            for i, box in enumerate(detections.xyxy):\n",
    "                color = colors[i] if i < len(colors) else sv.Color.GREEN\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color.as_bgr(), 2)\n",
    "\n",
    "            # Draw labels\n",
    "            # Draw smaller labels with smaller background rectangles\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            font_scale = 0.4  # significantly smaller\n",
    "            thickness = 1\n",
    "            pad = 3\n",
    "\n",
    "            for i, lbl in enumerate(labels):\n",
    "                x1, y1, x2, y2 = map(int, detections.xyxy[i])\n",
    "                text_size, _ = cv2.getTextSize(lbl, font, font_scale, thickness)\n",
    "                text_w, text_h = text_size\n",
    "\n",
    "                # Position label above box if space, otherwise below\n",
    "                if y1 - text_h - 2 * pad > 0:\n",
    "                    rect_tl = (x1, y1 - text_h - 2 * pad)\n",
    "                    rect_br = (x1 + text_w + 2 * pad, y1)\n",
    "                    text_org = (x1 + pad, y1 - pad)\n",
    "                else:\n",
    "                    rect_tl = (x1, y1)\n",
    "                    rect_br = (x1 + text_w + 2 * pad, y1 + text_h + 2 * pad)\n",
    "                    text_org = (x1 + pad, y1 + text_h + pad)\n",
    "\n",
    "                # Background color: use tracker color if available, else black\n",
    "                bg_color = colors[i].as_bgr() if i < len(colors) else (0, 0, 0)\n",
    "                # Choose text color for contrast\n",
    "                text_color = (0, 0, 0) if sum(bg_color) > 382 else (255, 255, 255)\n",
    "\n",
    "                cv2.rectangle(annotated_frame, rect_tl, rect_br, bg_color, cv2.FILLED)\n",
    "                cv2.putText(annotated_frame, lbl, text_org, font, font_scale, text_color, thickness, cv2.LINE_AA)\n",
    "\n",
    "            # Draw the counting line\n",
    "            cv2.line(annotated_frame, line_points[0], line_points[1], (0, 0, 255), 2)\n",
    "\n",
    "            # Overlay YOLO total count\n",
    "            cv2.putText(\n",
    "                annotated_frame,\n",
    "                f'Total Count: {total_count}',\n",
    "                (10, 50),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.5,\n",
    "                (0, 255, 0),\n",
    "                2,\n",
    "                cv2.LINE_AA\n",
    "            )\n",
    "            \n",
    "            # Overlay BBV total count\n",
    "            cv2.putText(\n",
    "                annotated_frame,\n",
    "                f'BBV Total Count: {total_count_bbv}',\n",
    "                (10, 80),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.5,\n",
    "                (255, 0, 0),\n",
    "                2,\n",
    "                cv2.LINE_AA\n",
    "            )\n",
    "            \n",
    "            # Overlay True total count\n",
    "            cv2.putText(\n",
    "                annotated_frame,\n",
    "                f'True Total Count: {TRUE_COUNT}',\n",
    "                (10, 110),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.5,\n",
    "                (0, 0, 255),\n",
    "                2,\n",
    "                cv2.LINE_AA\n",
    "            )\n",
    "\n",
    "            # Write out annotated frame\n",
    "            out.write(annotated_frame)\n",
    "            \n",
    "    except Exception as e:\n",
    "        # Detailed exception logging\n",
    "        print(\"=== Exception while processing video frames ===\")\n",
    "        print(\"Time:\", datetime.datetime.now().isoformat())\n",
    "        print(\"Exception type:\", type(e).__name__)\n",
    "        print(\"Exception message:\", str(e))\n",
    "        print(\"Full traceback:\")\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "    finally:\n",
    "        # Clean up writer and windows\n",
    "        out.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        print(f\"Total Frames Processed: {frame_count}\\nYOLO Count = {total_count}, BBV Total = {total_count_bbv}, True Count = {TRUE_COUNT}\")\n",
    "        if verbose:\n",
    "            print(f\"LineZone internal count (for reference): in={line_zone.in_count}, out={line_zone.out_count}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import tkinter as tk\n",
    "    from tkinter.filedialog import askopenfilename, askdirectory\n",
    "    tk.Tk().withdraw()\n",
    "\n",
    "    # Pick input video + output folder with file dialogs\n",
    "    SOURCE_VIDEO_PATH = askopenfilename()\n",
    "    print(\"User chose:\", SOURCE_VIDEO_PATH)\n",
    "\n",
    "    folder_path = askdirectory()\n",
    "    print(\"Output folder:\", folder_path)\n",
    "\n",
    "    # Build output filename\n",
    "    filename_no_ext = SOURCE_VIDEO_PATH.split('/')[-1].rsplit('.', 1)[0]\n",
    "    OUTPUT_PATH = f\"{folder_path}/{filename_no_ext}-outputfile(colored).mp4\"\n",
    "    print(\"Output path:\", OUTPUT_PATH)\n",
    "\n",
    "    # Grab a frame to define the line\n",
    "    cap = cv2.VideoCapture(SOURCE_VIDEO_PATH)\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to read the video\")\n",
    "        exit()\n",
    "    cap.release()\n",
    "\n",
    "    line_points = get_line_from_video_frame(frame)\n",
    "    print(\"Line points:\", line_points)\n",
    "\n",
    "    # Only run if line points are valid\n",
    "    if len(line_points) == 2:\n",
    "        chick_counting(SOURCE_VIDEO_PATH, OUTPUT_PATH, line_points, verbose=True)\n",
    "    else:\n",
    "        print(\"Error: Not enough points to define the counting line.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5280d8",
   "metadata": {},
   "source": [
    "## BBV Pipeline v2 - No Bounding Box Grouping\n",
    "\n",
    "Similar to the functionality above, however this will use a model that is trained on singular bounding boxes (no grouping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3082a59e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-3 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-3 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content {\n",
       "  display: none;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  overflow: visible;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-3 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-3 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-3 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-3 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".estimator-table summary {\n",
       "    padding: .5rem;\n",
       "    font-family: monospace;\n",
       "    cursor: pointer;\n",
       "}\n",
       "\n",
       ".estimator-table details[open] {\n",
       "    padding-left: 0.1rem;\n",
       "    padding-right: 0.1rem;\n",
       "    padding-bottom: 0.3rem;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table {\n",
       "    margin-left: auto !important;\n",
       "    margin-right: auto !important;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(odd) {\n",
       "    background-color: #fff;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(even) {\n",
       "    background-color: #f6f6f6;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:hover {\n",
       "    background-color: #e0e0e0;\n",
       "}\n",
       "\n",
       ".estimator-table table td {\n",
       "    border: 1px solid rgba(106, 105, 104, 0.232);\n",
       "}\n",
       "\n",
       ".user-set td {\n",
       "    color:rgb(255, 94, 0);\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td.value pre {\n",
       "    color:rgb(255, 94, 0) !important;\n",
       "    background-color: transparent !important;\n",
       "}\n",
       "\n",
       ".default td {\n",
       "    color: black;\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td i,\n",
       ".default td i {\n",
       "    color: black;\n",
       "}\n",
       "\n",
       ".copy-paste-icon {\n",
       "    background-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA0NDggNTEyIj48IS0tIUZvbnQgQXdlc29tZSBGcmVlIDYuNy4yIGJ5IEBmb250YXdlc29tZSAtIGh0dHBzOi8vZm9udGF3ZXNvbWUuY29tIExpY2Vuc2UgLSBodHRwczovL2ZvbnRhd2Vzb21lLmNvbS9saWNlbnNlL2ZyZWUgQ29weXJpZ2h0IDIwMjUgRm9udGljb25zLCBJbmMuLS0+PHBhdGggZD0iTTIwOCAwTDMzMi4xIDBjMTIuNyAwIDI0LjkgNS4xIDMzLjkgMTQuMWw2Ny45IDY3LjljOSA5IDE0LjEgMjEuMiAxNC4xIDMzLjlMNDQ4IDMzNmMwIDI2LjUtMjEuNSA0OC00OCA0OGwtMTkyIDBjLTI2LjUgMC00OC0yMS41LTQ4LTQ4bDAtMjg4YzAtMjYuNSAyMS41LTQ4IDQ4LTQ4ek00OCAxMjhsODAgMCAwIDY0LTY0IDAgMCAyNTYgMTkyIDAgMC0zMiA2NCAwIDAgNDhjMCAyNi41LTIxLjUgNDgtNDggNDhMNDggNTEyYy0yNi41IDAtNDgtMjEuNS00OC00OEwwIDE3NmMwLTI2LjUgMjEuNS00OCA0OC00OHoiLz48L3N2Zz4=);\n",
       "    background-repeat: no-repeat;\n",
       "    background-size: 14px 14px;\n",
       "    background-position: 0;\n",
       "    display: inline-block;\n",
       "    width: 14px;\n",
       "    height: 14px;\n",
       "    cursor: pointer;\n",
       "}\n",
       "</style><body><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;classifier&#x27;,\n",
       "                 RandomForestClassifier(max_depth=10, max_features=0.8,\n",
       "                                        min_samples_split=4))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>Pipeline</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.7/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('steps',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">steps&nbsp;</td>\n",
       "            <td class=\"value\">[(&#x27;scaler&#x27;, ...), (&#x27;classifier&#x27;, ...)]</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('transform_input',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">transform_input&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('memory',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">memory&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('verbose',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">verbose&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>StandardScaler</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.7/modules/generated/sklearn.preprocessing.StandardScaler.html\">?<span>Documentation for StandardScaler</span></a></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"scaler__\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('copy',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">copy&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('with_mean',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">with_mean&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('with_std',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">with_std&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>RandomForestClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.7/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"classifier__\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('n_estimators',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">n_estimators&nbsp;</td>\n",
       "            <td class=\"value\">100</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('criterion',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">criterion&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;gini&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_depth',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">max_depth&nbsp;</td>\n",
       "            <td class=\"value\">10</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('min_samples_split',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">min_samples_split&nbsp;</td>\n",
       "            <td class=\"value\">4</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('min_samples_leaf',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">min_samples_leaf&nbsp;</td>\n",
       "            <td class=\"value\">1</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('min_weight_fraction_leaf',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">min_weight_fraction_leaf&nbsp;</td>\n",
       "            <td class=\"value\">0.0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_features',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">max_features&nbsp;</td>\n",
       "            <td class=\"value\">0.8</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_leaf_nodes',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">max_leaf_nodes&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('min_impurity_decrease',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">min_impurity_decrease&nbsp;</td>\n",
       "            <td class=\"value\">0.0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('bootstrap',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">bootstrap&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('oob_score',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">oob_score&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('n_jobs',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">n_jobs&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('random_state',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">random_state&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('verbose',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">verbose&nbsp;</td>\n",
       "            <td class=\"value\">0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('warm_start',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">warm_start&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('class_weight',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">class_weight&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('ccp_alpha',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">ccp_alpha&nbsp;</td>\n",
       "            <td class=\"value\">0.0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_samples',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">max_samples&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('monotonic_cst',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">monotonic_cst&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div></div></div><script>function copyToClipboard(text, element) {\n",
       "    // Get the parameter prefix from the closest toggleable content\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;\n",
       "\n",
       "    const originalStyle = element.style;\n",
       "    const computedStyle = window.getComputedStyle(element);\n",
       "    const originalWidth = computedStyle.width;\n",
       "    const originalHTML = element.innerHTML.replace('Copied!', '');\n",
       "\n",
       "    navigator.clipboard.writeText(fullParamName)\n",
       "        .then(() => {\n",
       "            element.style.width = originalWidth;\n",
       "            element.style.color = 'green';\n",
       "            element.innerHTML = \"Copied!\";\n",
       "\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        })\n",
       "        .catch(err => {\n",
       "            console.error('Failed to copy:', err);\n",
       "            element.style.color = 'red';\n",
       "            element.innerHTML = \"Failed!\";\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        });\n",
       "    return false;\n",
       "}\n",
       "\n",
       "document.querySelectorAll('.fa-regular.fa-copy').forEach(function(element) {\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const paramName = element.parentElement.nextElementSibling.textContent.trim();\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;\n",
       "\n",
       "    element.setAttribute('title', fullParamName);\n",
       "});\n",
       "</script></body>"
      ],
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('classifier',\n",
       "                 RandomForestClassifier(max_depth=10, max_features=0.8,\n",
       "                                        min_samples_split=4))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Determine video/clip to test '''\n",
    "TRUE_COUNT = 75  # True count for the given clip\n",
    "\n",
    "''' Load the relevant models and scalers '''\n",
    "BBV_MODEL_NO_GROUPING = joblib.load('./models/thermal_chick_counting_NOGROUP_rf_model_fit.pkl')  # Load the pre-trained BBV model\n",
    "BBV_STANDARD_SCALER_NO_GROUPING = joblib.load('./models/thermal_chick_counting_NOGROUP_rf_scaler.pkl')  # Load the pre-fitted StandardScaler\n",
    "YOLO_MODEL = YOLO('./models/new_iron.pt')  # Load the pre-trained YOLO model\n",
    "\n",
    "# Declare the pipeline\n",
    "BBV_PIPELINE_NO_GROUPING = Pipeline([\n",
    "    ('scaler', BBV_STANDARD_SCALER_NO_GROUPING),\n",
    "    ('classifier', BBV_MODEL_NO_GROUPING)\n",
    "])\n",
    "\n",
    "# Validate that the model properly loaded\n",
    "BBV_PIPELINE_NO_GROUPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25c89e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User chose: /mnt/c/Users/JJ/Desktop/Repos/Chick-Counting/thermal/misc/BBV_Tests/True-Counts/Top Belt(Iron) 02 - CLEAN_TEST_TRUE_COUNT_75.mp4\n",
      "Output folder: /mnt/c/Users/JJ/Desktop/Repos/Chick-Counting/thermal/misc/BBV_Tests\n",
      "Output path: /mnt/c/Users/JJ/Desktop/Repos/Chick-Counting/thermal/misc/BBV_Tests/Top Belt(Iron) 02 - CLEAN_TEST_TRUE_COUNT_75-PIPELINETEST-outputfile(colored).mp4\n",
      "Line points: [(256, 171), (0, 171)]\n",
      "\n",
      "0: 640x480 1 Chick, 12.3ms\n",
      "Speed: 1.8ms preprocess, 12.3ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 1 Chick, 24.4ms\n",
      "Speed: 2.1ms preprocess, 24.4ms inference, 7.8ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 1 Chick, 10.5ms\n",
      "Speed: 1.8ms preprocess, 10.5ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 1 Chick, 11.7ms\n",
      "Speed: 1.6ms preprocess, 11.7ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 1 Chick, 11.1ms\n",
      "Speed: 1.6ms preprocess, 11.1ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 1 Chick, 9.1ms\n",
      "Speed: 1.6ms preprocess, 9.1ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 1 Chick, 24.0ms\n",
      "Speed: 1.7ms preprocess, 24.0ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 1 Chick, 9.8ms\n",
      "Speed: 1.8ms preprocess, 9.8ms inference, 3.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 1 Chick, 16.2ms\n",
      "Speed: 1.9ms preprocess, 16.2ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 1 Chick, 10.3ms\n",
      "Speed: 1.7ms preprocess, 10.3ms inference, 9.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 1 Chick, 9.2ms\n",
      "Speed: 1.5ms preprocess, 9.2ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 1 Chick, 10.0ms\n",
      "Speed: 1.9ms preprocess, 10.0ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 1 Chick, 23.0ms\n",
      "Speed: 2.0ms preprocess, 23.0ms inference, 7.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 1 Chick, 9.8ms\n",
      "Speed: 2.0ms preprocess, 9.8ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 1 Chick, 10.4ms\n",
      "Speed: 1.7ms preprocess, 10.4ms inference, 4.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 1 Chick, 10.4ms\n",
      "Speed: 1.8ms preprocess, 10.4ms inference, 3.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 1 Chick, 9.9ms\n",
      "Speed: 1.7ms preprocess, 9.9ms inference, 3.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 1 Chick, 12.4ms\n",
      "Speed: 1.5ms preprocess, 12.4ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 1 Chick, 9.6ms\n",
      "Speed: 1.4ms preprocess, 9.6ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 1 Chick, 9.8ms\n",
      "Speed: 1.6ms preprocess, 9.8ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 1 Chick, 10.2ms\n",
      "Speed: 1.8ms preprocess, 10.2ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 1 Chick, 9.7ms\n",
      "Speed: 1.6ms preprocess, 9.7ms inference, 2.8ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 1 Chick, 9.2ms\n",
      "Speed: 1.6ms preprocess, 9.2ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 1 Chick, 9.8ms\n",
      "Speed: 1.4ms preprocess, 9.8ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 1 Chick, 9.5ms\n",
      "Speed: 1.4ms preprocess, 9.5ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 3 Chicks, 9.8ms\n",
      "Speed: 1.4ms preprocess, 9.8ms inference, 4.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 2 Chicks, 9.8ms\n",
      "Speed: 1.5ms preprocess, 9.8ms inference, 2.8ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 3 Chicks, 9.2ms\n",
      "Speed: 1.5ms preprocess, 9.2ms inference, 8.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 5 Chicks, 9.6ms\n",
      "Speed: 1.4ms preprocess, 9.6ms inference, 6.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 7 Chicks, 8.9ms\n",
      "Speed: 1.7ms preprocess, 8.9ms inference, 7.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 5 Chicks, 9.4ms\n",
      "Speed: 1.3ms preprocess, 9.4ms inference, 9.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 5 Chicks, 9.3ms\n",
      "Speed: 1.4ms preprocess, 9.3ms inference, 5.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 4 Chicks, 20.4ms\n",
      "Speed: 1.5ms preprocess, 20.4ms inference, 11.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 6 Chicks, 20.6ms\n",
      "Speed: 1.9ms preprocess, 20.6ms inference, 15.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 6 Chicks, 9.1ms\n",
      "Speed: 1.6ms preprocess, 9.1ms inference, 6.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 6 Chicks, 19.9ms\n",
      "Speed: 1.4ms preprocess, 19.9ms inference, 15.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 7 Chicks, 20.2ms\n",
      "Speed: 1.5ms preprocess, 20.2ms inference, 16.8ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 6 Chicks, 16.9ms\n",
      "Speed: 1.6ms preprocess, 16.9ms inference, 5.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 11 Chicks, 8.8ms\n",
      "Speed: 1.7ms preprocess, 8.8ms inference, 10.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 5 Chicks, 20.1ms\n",
      "Speed: 1.9ms preprocess, 20.1ms inference, 16.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 11 Chicks, 8.8ms\n",
      "Speed: 1.3ms preprocess, 8.8ms inference, 11.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 5 Chicks, 16.7ms\n",
      "Speed: 1.6ms preprocess, 16.7ms inference, 7.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 6 Chicks, 15.9ms\n",
      "Speed: 1.4ms preprocess, 15.9ms inference, 6.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 6 Chicks, 8.9ms\n",
      "Speed: 1.7ms preprocess, 8.9ms inference, 7.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 5 Chicks, 16.7ms\n",
      "Speed: 1.5ms preprocess, 16.7ms inference, 9.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 5 Chicks, 20.5ms\n",
      "Speed: 1.4ms preprocess, 20.5ms inference, 9.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 6 Chicks, 10.2ms\n",
      "Speed: 1.5ms preprocess, 10.2ms inference, 9.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 5 Chicks, 18.1ms\n",
      "Speed: 1.5ms preprocess, 18.1ms inference, 6.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 5 Chicks, 20.9ms\n",
      "Speed: 1.5ms preprocess, 20.9ms inference, 16.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 5 Chicks, 20.0ms\n",
      "Speed: 1.8ms preprocess, 20.0ms inference, 16.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 8 Chicks, 20.1ms\n",
      "Speed: 1.4ms preprocess, 20.1ms inference, 21.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 6 Chicks, 12.8ms\n",
      "Speed: 1.4ms preprocess, 12.8ms inference, 10.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 6 Chicks, 20.0ms\n",
      "Speed: 1.5ms preprocess, 20.0ms inference, 17.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 6 Chicks, 20.4ms\n",
      "Speed: 1.6ms preprocess, 20.4ms inference, 18.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 6 Chicks, 20.6ms\n",
      "Speed: 1.5ms preprocess, 20.6ms inference, 9.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 11 Chicks, 10.0ms\n",
      "Speed: 1.4ms preprocess, 10.0ms inference, 13.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 9 Chicks, 8.7ms\n",
      "Speed: 1.5ms preprocess, 8.7ms inference, 9.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 7 Chicks, 20.0ms\n",
      "Speed: 1.5ms preprocess, 20.0ms inference, 20.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 7 Chicks, 17.6ms\n",
      "Speed: 1.5ms preprocess, 17.6ms inference, 7.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 8 Chicks, 20.0ms\n",
      "Speed: 1.4ms preprocess, 20.0ms inference, 20.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 7 Chicks, 8.8ms\n",
      "Speed: 1.4ms preprocess, 8.8ms inference, 8.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 7 Chicks, 20.9ms\n",
      "Speed: 1.6ms preprocess, 20.9ms inference, 20.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 7 Chicks, 12.0ms\n",
      "Speed: 1.4ms preprocess, 12.0ms inference, 8.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 11 Chicks, 20.1ms\n",
      "Speed: 1.5ms preprocess, 20.1ms inference, 27.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 6 Chicks, 20.1ms\n",
      "Speed: 1.8ms preprocess, 20.1ms inference, 6.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 6 Chicks, 12.5ms\n",
      "Speed: 1.5ms preprocess, 12.5ms inference, 7.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 6 Chicks, 15.4ms\n",
      "Speed: 2.0ms preprocess, 15.4ms inference, 7.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 6 Chicks, 20.3ms\n",
      "Speed: 1.4ms preprocess, 20.3ms inference, 17.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 6 Chicks, 8.8ms\n",
      "Speed: 1.9ms preprocess, 8.8ms inference, 9.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 10 Chicks, 9.3ms\n",
      "Speed: 1.4ms preprocess, 9.3ms inference, 10.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 8 Chicks, 9.3ms\n",
      "Speed: 1.4ms preprocess, 9.3ms inference, 9.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 9 Chicks, 9.4ms\n",
      "Speed: 1.4ms preprocess, 9.4ms inference, 9.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 7 Chicks, 9.4ms\n",
      "Speed: 1.4ms preprocess, 9.4ms inference, 8.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 8 Chicks, 9.4ms\n",
      "Speed: 1.5ms preprocess, 9.4ms inference, 8.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 7 Chicks, 21.1ms\n",
      "Speed: 1.5ms preprocess, 21.1ms inference, 20.8ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 7 Chicks, 9.4ms\n",
      "Speed: 1.3ms preprocess, 9.4ms inference, 7.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 9 Chicks, 10.7ms\n",
      "Speed: 1.6ms preprocess, 10.7ms inference, 14.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 10 Chicks, 8.7ms\n",
      "Speed: 1.7ms preprocess, 8.7ms inference, 10.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 9 Chicks, 9.4ms\n",
      "Speed: 1.3ms preprocess, 9.4ms inference, 9.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 10 Chicks, 9.4ms\n",
      "Speed: 1.3ms preprocess, 9.4ms inference, 10.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 11 Chicks, 9.5ms\n",
      "Speed: 1.3ms preprocess, 9.5ms inference, 10.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 8 Chicks, 20.4ms\n",
      "Speed: 1.4ms preprocess, 20.4ms inference, 23.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 11 Chicks, 8.9ms\n",
      "Speed: 1.5ms preprocess, 8.9ms inference, 10.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 11 Chicks, 8.9ms\n",
      "Speed: 1.5ms preprocess, 8.9ms inference, 11.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 9 Chicks, 17.1ms\n",
      "Speed: 1.4ms preprocess, 17.1ms inference, 9.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 10 Chicks, 20.0ms\n",
      "Speed: 1.9ms preprocess, 20.0ms inference, 26.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 13 Chicks, 11.3ms\n",
      "Speed: 1.9ms preprocess, 11.3ms inference, 12.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 8 Chicks, 9.2ms\n",
      "Speed: 1.3ms preprocess, 9.2ms inference, 10.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 9 Chicks, 20.6ms\n",
      "Speed: 1.5ms preprocess, 20.6ms inference, 23.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 14 Chicks, 9.2ms\n",
      "Speed: 1.4ms preprocess, 9.2ms inference, 14.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 20 Chicks, 20.2ms\n",
      "Speed: 1.5ms preprocess, 20.2ms inference, 47.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 11 Chicks, 10.6ms\n",
      "Speed: 1.4ms preprocess, 10.6ms inference, 15.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 13 Chicks, 10.5ms\n",
      "Speed: 1.5ms preprocess, 10.5ms inference, 12.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 7 Chicks, 12.2ms\n",
      "Speed: 1.5ms preprocess, 12.2ms inference, 8.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 8 Chicks, 20.2ms\n",
      "Speed: 1.5ms preprocess, 20.2ms inference, 20.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 10 Chicks, 20.1ms\n",
      "Speed: 1.5ms preprocess, 20.1ms inference, 25.8ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 7 Chicks, 12.7ms\n",
      "Speed: 1.4ms preprocess, 12.7ms inference, 11.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 8 Chicks, 8.8ms\n",
      "Speed: 1.5ms preprocess, 8.8ms inference, 8.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 12 Chicks, 8.8ms\n",
      "Speed: 1.5ms preprocess, 8.8ms inference, 11.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 9 Chicks, 19.7ms\n",
      "Speed: 1.5ms preprocess, 19.7ms inference, 22.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 14 Chicks, 20.2ms\n",
      "Speed: 1.4ms preprocess, 20.2ms inference, 12.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 14 Chicks, 9.4ms\n",
      "Speed: 1.6ms preprocess, 9.4ms inference, 14.8ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 22 Chicks, 9.6ms\n",
      "Speed: 1.4ms preprocess, 9.6ms inference, 20.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 12 Chicks, 9.1ms\n",
      "Speed: 1.4ms preprocess, 9.1ms inference, 32.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 13 Chicks, 11.4ms\n",
      "Speed: 1.5ms preprocess, 11.4ms inference, 19.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 7 Chicks, 14.6ms\n",
      "Speed: 1.6ms preprocess, 14.6ms inference, 12.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 18 Chicks, 20.7ms\n",
      "Speed: 1.7ms preprocess, 20.7ms inference, 17.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 16 Chicks, 20.5ms\n",
      "Speed: 1.6ms preprocess, 20.5ms inference, 26.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 20 Chicks, 10.0ms\n",
      "Speed: 1.6ms preprocess, 10.0ms inference, 18.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 12 Chicks, 20.9ms\n",
      "Speed: 1.5ms preprocess, 20.9ms inference, 15.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 10 Chicks, 9.7ms\n",
      "Speed: 1.6ms preprocess, 9.7ms inference, 10.8ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 9 Chicks, 20.6ms\n",
      "Speed: 1.6ms preprocess, 20.6ms inference, 22.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 11 Chicks, 12.7ms\n",
      "Speed: 1.7ms preprocess, 12.7ms inference, 10.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 9 Chicks, 20.8ms\n",
      "Speed: 1.9ms preprocess, 20.8ms inference, 26.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 11 Chicks, 20.7ms\n",
      "Speed: 2.0ms preprocess, 20.7ms inference, 13.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 14 Chicks, 20.6ms\n",
      "Speed: 1.5ms preprocess, 20.6ms inference, 36.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 8 Chicks, 9.2ms\n",
      "Speed: 1.4ms preprocess, 9.2ms inference, 9.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 8 Chicks, 21.1ms\n",
      "Speed: 1.8ms preprocess, 21.1ms inference, 24.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 8 Chicks, 20.8ms\n",
      "Speed: 2.0ms preprocess, 20.8ms inference, 14.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 8 Chicks, 21.2ms\n",
      "Speed: 1.7ms preprocess, 21.2ms inference, 24.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 9 Chicks, 15.6ms\n",
      "Speed: 1.5ms preprocess, 15.6ms inference, 24.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 8 Chicks, 9.7ms\n",
      "Speed: 1.5ms preprocess, 9.7ms inference, 9.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 12 Chicks, 14.8ms\n",
      "Speed: 1.6ms preprocess, 14.8ms inference, 14.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 17 Chicks, 20.8ms\n",
      "Speed: 1.6ms preprocess, 20.8ms inference, 30.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 9 Chicks, 20.6ms\n",
      "Speed: 1.5ms preprocess, 20.6ms inference, 23.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 9 Chicks, 9.0ms\n",
      "Speed: 1.7ms preprocess, 9.0ms inference, 11.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 8 Chicks, 9.1ms\n",
      "Speed: 1.7ms preprocess, 9.1ms inference, 9.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 8 Chicks, 9.2ms\n",
      "Speed: 1.4ms preprocess, 9.2ms inference, 10.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 8 Chicks, 9.6ms\n",
      "Speed: 1.4ms preprocess, 9.6ms inference, 8.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 9 Chicks, 9.2ms\n",
      "Speed: 1.5ms preprocess, 9.2ms inference, 11.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 11 Chicks, 9.7ms\n",
      "Speed: 1.4ms preprocess, 9.7ms inference, 11.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 11 Chicks, 9.8ms\n",
      "Speed: 1.6ms preprocess, 9.8ms inference, 23.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 9 Chicks, 9.6ms\n",
      "Speed: 1.4ms preprocess, 9.6ms inference, 10.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 9 Chicks, 9.4ms\n",
      "Speed: 1.4ms preprocess, 9.4ms inference, 10.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 10 Chicks, 9.6ms\n",
      "Speed: 1.4ms preprocess, 9.6ms inference, 9.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 13 Chicks, 9.6ms\n",
      "Speed: 1.5ms preprocess, 9.6ms inference, 12.8ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 13 Chicks, 9.1ms\n",
      "Speed: 1.5ms preprocess, 9.1ms inference, 13.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 15 Chicks, 9.5ms\n",
      "Speed: 1.4ms preprocess, 9.5ms inference, 14.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 18 Chicks, 9.5ms\n",
      "Speed: 1.4ms preprocess, 9.5ms inference, 17.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 23 Chicks, 22.5ms\n",
      "Speed: 1.7ms preprocess, 22.5ms inference, 38.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 16 Chicks, 9.6ms\n",
      "Speed: 1.5ms preprocess, 9.6ms inference, 15.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 14 Chicks, 9.1ms\n",
      "Speed: 1.5ms preprocess, 9.1ms inference, 14.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 17 Chicks, 9.1ms\n",
      "Speed: 1.5ms preprocess, 9.1ms inference, 16.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 14 Chicks, 9.0ms\n",
      "Speed: 1.5ms preprocess, 9.0ms inference, 17.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 12 Chicks, 9.5ms\n",
      "Speed: 1.4ms preprocess, 9.5ms inference, 11.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 14 Chicks, 9.6ms\n",
      "Speed: 1.5ms preprocess, 9.6ms inference, 14.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 10 Chicks, 9.5ms\n",
      "Speed: 1.4ms preprocess, 9.5ms inference, 11.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 11 Chicks, 9.7ms\n",
      "Speed: 1.4ms preprocess, 9.7ms inference, 12.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 13 Chicks, 9.4ms\n",
      "Speed: 1.4ms preprocess, 9.4ms inference, 12.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 19 Chicks, 9.6ms\n",
      "Speed: 1.4ms preprocess, 9.6ms inference, 18.8ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 14 Chicks, 9.1ms\n",
      "Speed: 1.4ms preprocess, 9.1ms inference, 16.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 20 Chicks, 9.2ms\n",
      "Speed: 1.5ms preprocess, 9.2ms inference, 19.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 18 Chicks, 9.1ms\n",
      "Speed: 1.5ms preprocess, 9.1ms inference, 18.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 15 Chicks, 9.4ms\n",
      "Speed: 1.4ms preprocess, 9.4ms inference, 15.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 15 Chicks, 9.6ms\n",
      "Speed: 1.5ms preprocess, 9.6ms inference, 14.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 12 Chicks, 9.6ms\n",
      "Speed: 1.4ms preprocess, 9.6ms inference, 12.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 11 Chicks, 9.3ms\n",
      "Speed: 1.5ms preprocess, 9.3ms inference, 12.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 16 Chicks, 9.7ms\n",
      "Speed: 1.5ms preprocess, 9.7ms inference, 24.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 17 Chicks, 9.1ms\n",
      "Speed: 1.6ms preprocess, 9.1ms inference, 16.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 17 Chicks, 9.7ms\n",
      "Speed: 1.5ms preprocess, 9.7ms inference, 16.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 18 Chicks, 9.5ms\n",
      "Speed: 1.5ms preprocess, 9.5ms inference, 17.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 12 Chicks, 9.4ms\n",
      "Speed: 1.9ms preprocess, 9.4ms inference, 14.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 15 Chicks, 9.3ms\n",
      "Speed: 1.5ms preprocess, 9.3ms inference, 38.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 14 Chicks, 11.9ms\n",
      "Speed: 1.6ms preprocess, 11.9ms inference, 15.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 15 Chicks, 9.7ms\n",
      "Speed: 1.3ms preprocess, 9.7ms inference, 14.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 18 Chicks, 9.6ms\n",
      "Speed: 1.4ms preprocess, 9.6ms inference, 18.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 31 Chicks, 9.1ms\n",
      "Speed: 1.6ms preprocess, 9.1ms inference, 28.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 28 Chicks, 9.7ms\n",
      "Speed: 1.4ms preprocess, 9.7ms inference, 27.8ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 22 Chicks, 9.2ms\n",
      "Speed: 1.5ms preprocess, 9.2ms inference, 19.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 14 Chicks, 9.6ms\n",
      "Speed: 1.6ms preprocess, 9.6ms inference, 25.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 24 Chicks, 22.1ms\n",
      "Speed: 1.6ms preprocess, 22.1ms inference, 60.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 17 Chicks, 9.5ms\n",
      "Speed: 1.4ms preprocess, 9.5ms inference, 16.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 13 Chicks, 9.6ms\n",
      "Speed: 1.4ms preprocess, 9.6ms inference, 13.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 15 Chicks, 9.6ms\n",
      "Speed: 1.5ms preprocess, 9.6ms inference, 24.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 11 Chicks, 9.1ms\n",
      "Speed: 1.5ms preprocess, 9.1ms inference, 19.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 20 Chicks, 9.3ms\n",
      "Speed: 1.7ms preprocess, 9.3ms inference, 18.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 20 Chicks, 9.5ms\n",
      "Speed: 1.5ms preprocess, 9.5ms inference, 18.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 14 Chicks, 9.1ms\n",
      "Speed: 1.5ms preprocess, 9.1ms inference, 14.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 15 Chicks, 9.0ms\n",
      "Speed: 1.6ms preprocess, 9.0ms inference, 14.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 14 Chicks, 9.3ms\n",
      "Speed: 1.5ms preprocess, 9.3ms inference, 14.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 15 Chicks, 9.2ms\n",
      "Speed: 1.3ms preprocess, 9.2ms inference, 14.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 18 Chicks, 9.7ms\n",
      "Speed: 1.4ms preprocess, 9.7ms inference, 18.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 15 Chicks, 9.6ms\n",
      "Speed: 1.4ms preprocess, 9.6ms inference, 20.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 16 Chicks, 9.0ms\n",
      "Speed: 1.4ms preprocess, 9.0ms inference, 15.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 15 Chicks, 9.1ms\n",
      "Speed: 1.4ms preprocess, 9.1ms inference, 17.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 18 Chicks, 9.5ms\n",
      "Speed: 1.7ms preprocess, 9.5ms inference, 24.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 21 Chicks, 9.1ms\n",
      "Speed: 1.4ms preprocess, 9.1ms inference, 23.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 20 Chicks, 9.8ms\n",
      "Speed: 1.4ms preprocess, 9.8ms inference, 23.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 18 Chicks, 9.1ms\n",
      "Speed: 1.7ms preprocess, 9.1ms inference, 17.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 14 Chicks, 9.1ms\n",
      "Speed: 1.4ms preprocess, 9.1ms inference, 15.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 14 Chicks, 9.2ms\n",
      "Speed: 1.6ms preprocess, 9.2ms inference, 14.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 16 Chicks, 9.2ms\n",
      "Speed: 1.4ms preprocess, 9.2ms inference, 15.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 16 Chicks, 10.1ms\n",
      "Speed: 1.4ms preprocess, 10.1ms inference, 14.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 19 Chicks, 9.8ms\n",
      "Speed: 1.5ms preprocess, 9.8ms inference, 18.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 20 Chicks, 11.3ms\n",
      "Speed: 1.5ms preprocess, 11.3ms inference, 29.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 29 Chicks, 9.1ms\n",
      "Speed: 1.5ms preprocess, 9.1ms inference, 26.8ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 19 Chicks, 9.6ms\n",
      "Speed: 1.4ms preprocess, 9.6ms inference, 16.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 28 Chicks, 9.7ms\n",
      "Speed: 1.4ms preprocess, 9.7ms inference, 25.8ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 27 Chicks, 9.2ms\n",
      "Speed: 1.6ms preprocess, 9.2ms inference, 26.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 34 Chicks, 9.2ms\n",
      "Speed: 1.8ms preprocess, 9.2ms inference, 41.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 31 Chicks, 9.2ms\n",
      "Speed: 1.4ms preprocess, 9.2ms inference, 56.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 25 Chicks, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 55.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 19 Chicks, 11.4ms\n",
      "Speed: 1.7ms preprocess, 11.4ms inference, 21.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 24 Chicks, 9.8ms\n",
      "Speed: 1.5ms preprocess, 9.8ms inference, 50.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 35 Chicks, 9.3ms\n",
      "Speed: 1.9ms preprocess, 9.3ms inference, 32.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 28 Chicks, 9.5ms\n",
      "Speed: 1.5ms preprocess, 9.5ms inference, 27.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 24 Chicks, 14.2ms\n",
      "Speed: 1.7ms preprocess, 14.2ms inference, 23.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 21 Chicks, 9.7ms\n",
      "Speed: 1.4ms preprocess, 9.7ms inference, 21.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 22 Chicks, 9.1ms\n",
      "Speed: 1.7ms preprocess, 9.1ms inference, 21.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 21 Chicks, 9.6ms\n",
      "Speed: 1.7ms preprocess, 9.6ms inference, 21.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 23 Chicks, 9.5ms\n",
      "Speed: 1.5ms preprocess, 9.5ms inference, 21.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 28 Chicks, 9.2ms\n",
      "Speed: 1.6ms preprocess, 9.2ms inference, 27.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 25 Chicks, 9.8ms\n",
      "Speed: 1.4ms preprocess, 9.8ms inference, 23.8ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 29 Chicks, 9.7ms\n",
      "Speed: 1.4ms preprocess, 9.7ms inference, 28.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 31 Chicks, 9.7ms\n",
      "Speed: 1.6ms preprocess, 9.7ms inference, 77.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 31 Chicks, 9.1ms\n",
      "Speed: 1.6ms preprocess, 9.1ms inference, 31.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 26 Chicks, 9.2ms\n",
      "Speed: 1.4ms preprocess, 9.2ms inference, 24.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 32 Chicks, 9.4ms\n",
      "Speed: 1.5ms preprocess, 9.4ms inference, 43.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 30 Chicks, 22.4ms\n",
      "Speed: 1.6ms preprocess, 22.4ms inference, 40.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 32 Chicks, 9.1ms\n",
      "Speed: 1.4ms preprocess, 9.1ms inference, 35.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 30 Chicks, 9.3ms\n",
      "Speed: 1.6ms preprocess, 9.3ms inference, 26.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 34 Chicks, 9.6ms\n",
      "Speed: 1.6ms preprocess, 9.6ms inference, 36.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 36 Chicks, 9.6ms\n",
      "Speed: 1.9ms preprocess, 9.6ms inference, 49.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 33 Chicks, 10.4ms\n",
      "Speed: 1.5ms preprocess, 10.4ms inference, 33.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 21 Chicks, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 20.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 24 Chicks, 9.2ms\n",
      "Speed: 1.5ms preprocess, 9.2ms inference, 22.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 25 Chicks, 9.9ms\n",
      "Speed: 1.6ms preprocess, 9.9ms inference, 24.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 21 Chicks, 9.7ms\n",
      "Speed: 1.4ms preprocess, 9.7ms inference, 19.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 21 Chicks, 9.8ms\n",
      "Speed: 1.5ms preprocess, 9.8ms inference, 20.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 21 Chicks, 9.2ms\n",
      "Speed: 1.5ms preprocess, 9.2ms inference, 24.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 44 Chicks, 9.9ms\n",
      "Speed: 1.6ms preprocess, 9.9ms inference, 39.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 39 Chicks, 15.6ms\n",
      "Speed: 1.6ms preprocess, 15.6ms inference, 54.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 27 Chicks, 9.7ms\n",
      "Speed: 1.6ms preprocess, 9.7ms inference, 25.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 27 Chicks, 10.2ms\n",
      "Speed: 1.5ms preprocess, 10.2ms inference, 32.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 41 Chicks, 10.2ms\n",
      "Speed: 1.8ms preprocess, 10.2ms inference, 54.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 40 Chicks, 9.8ms\n",
      "Speed: 1.5ms preprocess, 9.8ms inference, 45.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 38 Chicks, 9.7ms\n",
      "Speed: 1.5ms preprocess, 9.7ms inference, 35.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 34 Chicks, 9.4ms\n",
      "Speed: 1.5ms preprocess, 9.4ms inference, 31.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 43 Chicks, 9.8ms\n",
      "Speed: 1.5ms preprocess, 9.8ms inference, 40.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 48 Chicks, 9.4ms\n",
      "Speed: 1.7ms preprocess, 9.4ms inference, 45.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 50 Chicks, 9.4ms\n",
      "Speed: 1.7ms preprocess, 9.4ms inference, 73.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 33 Chicks, 9.2ms\n",
      "Speed: 1.8ms preprocess, 9.2ms inference, 29.8ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 31 Chicks, 9.2ms\n",
      "Speed: 1.7ms preprocess, 9.2ms inference, 34.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 37 Chicks, 9.3ms\n",
      "Speed: 1.5ms preprocess, 9.3ms inference, 32.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 40 Chicks, 9.8ms\n",
      "Speed: 1.5ms preprocess, 9.8ms inference, 38.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 33 Chicks, 9.1ms\n",
      "Speed: 1.4ms preprocess, 9.1ms inference, 30.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 42 Chicks, 9.1ms\n",
      "Speed: 1.4ms preprocess, 9.1ms inference, 38.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 32 Chicks, 9.3ms\n",
      "Speed: 1.5ms preprocess, 9.3ms inference, 28.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 33 Chicks, 16.4ms\n",
      "Speed: 1.6ms preprocess, 16.4ms inference, 36.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 35 Chicks, 9.6ms\n",
      "Speed: 1.6ms preprocess, 9.6ms inference, 31.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 36 Chicks, 9.1ms\n",
      "Speed: 1.4ms preprocess, 9.1ms inference, 33.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 34 Chicks, 11.7ms\n",
      "Speed: 1.6ms preprocess, 11.7ms inference, 31.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 37 Chicks, 9.7ms\n",
      "Speed: 1.5ms preprocess, 9.7ms inference, 49.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 32 Chicks, 9.3ms\n",
      "Speed: 1.7ms preprocess, 9.3ms inference, 45.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 39 Chicks, 9.7ms\n",
      "Speed: 1.6ms preprocess, 9.7ms inference, 50.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 46 Chicks, 9.6ms\n",
      "Speed: 1.4ms preprocess, 9.6ms inference, 42.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 40 Chicks, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 37.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 32 Chicks, 9.5ms\n",
      "Speed: 1.5ms preprocess, 9.5ms inference, 31.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 32 Chicks, 9.2ms\n",
      "Speed: 1.4ms preprocess, 9.2ms inference, 31.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 33 Chicks, 9.8ms\n",
      "Speed: 1.5ms preprocess, 9.8ms inference, 27.8ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 30 Chicks, 9.2ms\n",
      "Speed: 1.6ms preprocess, 9.2ms inference, 27.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 29 Chicks, 9.0ms\n",
      "Speed: 1.5ms preprocess, 9.0ms inference, 42.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 34 Chicks, 9.2ms\n",
      "Speed: 1.5ms preprocess, 9.2ms inference, 29.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 34 Chicks, 9.8ms\n",
      "Speed: 1.5ms preprocess, 9.8ms inference, 32.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 38 Chicks, 9.1ms\n",
      "Speed: 1.7ms preprocess, 9.1ms inference, 35.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 30 Chicks, 9.1ms\n",
      "Speed: 1.7ms preprocess, 9.1ms inference, 28.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 32 Chicks, 9.8ms\n",
      "Speed: 1.4ms preprocess, 9.8ms inference, 29.8ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 31 Chicks, 10.0ms\n",
      "Speed: 1.5ms preprocess, 10.0ms inference, 30.8ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 35 Chicks, 10.4ms\n",
      "Speed: 1.5ms preprocess, 10.4ms inference, 34.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 33 Chicks, 9.7ms\n",
      "Speed: 1.8ms preprocess, 9.7ms inference, 37.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 36 Chicks, 9.6ms\n",
      "Speed: 1.8ms preprocess, 9.6ms inference, 37.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 39 Chicks, 10.2ms\n",
      "Speed: 1.6ms preprocess, 10.2ms inference, 39.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 28 Chicks, 10.3ms\n",
      "Speed: 1.8ms preprocess, 10.3ms inference, 28.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 27 Chicks, 10.0ms\n",
      "Speed: 1.5ms preprocess, 10.0ms inference, 36.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 30 Chicks, 9.5ms\n",
      "Speed: 1.5ms preprocess, 9.5ms inference, 34.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 27 Chicks, 10.0ms\n",
      "Speed: 1.5ms preprocess, 10.0ms inference, 24.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 24 Chicks, 10.1ms\n",
      "Speed: 1.4ms preprocess, 10.1ms inference, 21.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 25 Chicks, 10.1ms\n",
      "Speed: 1.4ms preprocess, 10.1ms inference, 51.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 28 Chicks, 10.1ms\n",
      "Speed: 1.6ms preprocess, 10.1ms inference, 26.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 23 Chicks, 10.2ms\n",
      "Speed: 1.4ms preprocess, 10.2ms inference, 21.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 20 Chicks, 9.5ms\n",
      "Speed: 1.6ms preprocess, 9.5ms inference, 18.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 16 Chicks, 10.1ms\n",
      "Speed: 1.5ms preprocess, 10.1ms inference, 18.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 18 Chicks, 9.6ms\n",
      "Speed: 1.5ms preprocess, 9.6ms inference, 17.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 18 Chicks, 9.8ms\n",
      "Speed: 1.6ms preprocess, 9.8ms inference, 19.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 20 Chicks, 10.0ms\n",
      "Speed: 1.4ms preprocess, 10.0ms inference, 19.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 18 Chicks, 9.4ms\n",
      "Speed: 1.9ms preprocess, 9.4ms inference, 18.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 23 Chicks, 10.1ms\n",
      "Speed: 1.5ms preprocess, 10.1ms inference, 21.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 28 Chicks, 9.6ms\n",
      "Speed: 1.7ms preprocess, 9.6ms inference, 28.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 34 Chicks, 9.6ms\n",
      "Speed: 1.5ms preprocess, 9.6ms inference, 32.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 38 Chicks, 9.7ms\n",
      "Speed: 1.6ms preprocess, 9.7ms inference, 39.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 35 Chicks, 9.5ms\n",
      "Speed: 1.4ms preprocess, 9.5ms inference, 34.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 26 Chicks, 9.6ms\n",
      "Speed: 2.0ms preprocess, 9.6ms inference, 25.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 30 Chicks, 9.6ms\n",
      "Speed: 2.0ms preprocess, 9.6ms inference, 29.8ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 29 Chicks, 10.6ms\n",
      "Speed: 1.6ms preprocess, 10.6ms inference, 34.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 36 Chicks, 10.3ms\n",
      "Speed: 1.5ms preprocess, 10.3ms inference, 35.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 29 Chicks, 10.2ms\n",
      "Speed: 1.5ms preprocess, 10.2ms inference, 44.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 31 Chicks, 10.1ms\n",
      "Speed: 1.5ms preprocess, 10.1ms inference, 31.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 33 Chicks, 9.7ms\n",
      "Speed: 1.7ms preprocess, 9.7ms inference, 32.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 44 Chicks, 9.6ms\n",
      "Speed: 1.5ms preprocess, 9.6ms inference, 41.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 45 Chicks, 10.4ms\n",
      "Speed: 1.6ms preprocess, 10.4ms inference, 49.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 34 Chicks, 9.7ms\n",
      "Speed: 1.6ms preprocess, 9.7ms inference, 33.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 36 Chicks, 9.3ms\n",
      "Speed: 1.7ms preprocess, 9.3ms inference, 33.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 35 Chicks, 11.1ms\n",
      "Speed: 2.1ms preprocess, 11.1ms inference, 49.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 35 Chicks, 11.1ms\n",
      "Speed: 1.9ms preprocess, 11.1ms inference, 33.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 42 Chicks, 9.8ms\n",
      "Speed: 1.4ms preprocess, 9.8ms inference, 48.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 39 Chicks, 9.5ms\n",
      "Speed: 1.4ms preprocess, 9.5ms inference, 35.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 47 Chicks, 9.3ms\n",
      "Speed: 1.5ms preprocess, 9.3ms inference, 44.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 52 Chicks, 9.1ms\n",
      "Speed: 1.6ms preprocess, 9.1ms inference, 47.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 34 Chicks, 9.2ms\n",
      "Speed: 1.5ms preprocess, 9.2ms inference, 31.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 40 Chicks, 9.0ms\n",
      "Speed: 1.7ms preprocess, 9.0ms inference, 39.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 41 Chicks, 9.5ms\n",
      "Speed: 1.6ms preprocess, 9.5ms inference, 59.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 36 Chicks, 10.3ms\n",
      "Speed: 1.5ms preprocess, 10.3ms inference, 35.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 27 Chicks, 9.1ms\n",
      "Speed: 1.8ms preprocess, 9.1ms inference, 25.8ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 35 Chicks, 10.5ms\n",
      "Speed: 1.4ms preprocess, 10.5ms inference, 51.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 30 Chicks, 9.9ms\n",
      "Speed: 1.4ms preprocess, 9.9ms inference, 28.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 35 Chicks, 9.7ms\n",
      "Speed: 1.4ms preprocess, 9.7ms inference, 31.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 23 Chicks, 9.3ms\n",
      "Speed: 1.5ms preprocess, 9.3ms inference, 23.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 36 Chicks, 13.4ms\n",
      "Speed: 1.5ms preprocess, 13.4ms inference, 35.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 34 Chicks, 10.3ms\n",
      "Speed: 1.4ms preprocess, 10.3ms inference, 30.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 33 Chicks, 9.1ms\n",
      "Speed: 1.4ms preprocess, 9.1ms inference, 33.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 30 Chicks, 9.1ms\n",
      "Speed: 1.6ms preprocess, 9.1ms inference, 28.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 30 Chicks, 9.5ms\n",
      "Speed: 1.5ms preprocess, 9.5ms inference, 27.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 30 Chicks, 9.2ms\n",
      "Speed: 1.6ms preprocess, 9.2ms inference, 28.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 26 Chicks, 9.1ms\n",
      "Speed: 1.5ms preprocess, 9.1ms inference, 24.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 21 Chicks, 9.2ms\n",
      "Speed: 1.6ms preprocess, 9.2ms inference, 18.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 31 Chicks, 9.0ms\n",
      "Speed: 1.6ms preprocess, 9.0ms inference, 36.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 29 Chicks, 9.3ms\n",
      "Speed: 1.5ms preprocess, 9.3ms inference, 37.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 25 Chicks, 9.6ms\n",
      "Speed: 1.6ms preprocess, 9.6ms inference, 25.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 23 Chicks, 9.8ms\n",
      "Speed: 1.5ms preprocess, 9.8ms inference, 43.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 33 Chicks, 9.7ms\n",
      "Speed: 1.4ms preprocess, 9.7ms inference, 32.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 33 Chicks, 9.3ms\n",
      "Speed: 1.5ms preprocess, 9.3ms inference, 30.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 27 Chicks, 9.6ms\n",
      "Speed: 1.4ms preprocess, 9.6ms inference, 25.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 24 Chicks, 9.7ms\n",
      "Speed: 1.4ms preprocess, 9.7ms inference, 22.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 23 Chicks, 9.6ms\n",
      "Speed: 1.5ms preprocess, 9.6ms inference, 22.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 22 Chicks, 9.0ms\n",
      "Speed: 1.5ms preprocess, 9.0ms inference, 20.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 22 Chicks, 9.0ms\n",
      "Speed: 1.5ms preprocess, 9.0ms inference, 20.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 21 Chicks, 9.6ms\n",
      "Speed: 1.4ms preprocess, 9.6ms inference, 20.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 23 Chicks, 9.3ms\n",
      "Speed: 1.4ms preprocess, 9.3ms inference, 23.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 24 Chicks, 9.5ms\n",
      "Speed: 1.4ms preprocess, 9.5ms inference, 22.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 25 Chicks, 20.6ms\n",
      "Speed: 1.5ms preprocess, 20.6ms inference, 30.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 30 Chicks, 19.7ms\n",
      "Speed: 1.3ms preprocess, 19.7ms inference, 52.8ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 27 Chicks, 8.9ms\n",
      "Speed: 1.4ms preprocess, 8.9ms inference, 42.8ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 31 Chicks, 9.4ms\n",
      "Speed: 1.4ms preprocess, 9.4ms inference, 38.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 35 Chicks, 10.9ms\n",
      "Speed: 2.3ms preprocess, 10.9ms inference, 45.8ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 29 Chicks, 9.9ms\n",
      "Speed: 1.8ms preprocess, 9.9ms inference, 40.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 24 Chicks, 9.3ms\n",
      "Speed: 1.5ms preprocess, 9.3ms inference, 25.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 32 Chicks, 8.8ms\n",
      "Speed: 1.3ms preprocess, 8.8ms inference, 28.8ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 32 Chicks, 19.2ms\n",
      "Speed: 1.4ms preprocess, 19.2ms inference, 32.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 30 Chicks, 9.5ms\n",
      "Speed: 1.3ms preprocess, 9.5ms inference, 25.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 34 Chicks, 9.4ms\n",
      "Speed: 1.4ms preprocess, 9.4ms inference, 30.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 32 Chicks, 9.3ms\n",
      "Speed: 1.5ms preprocess, 9.3ms inference, 55.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 31 Chicks, 9.3ms\n",
      "Speed: 1.3ms preprocess, 9.3ms inference, 28.8ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 34 Chicks, 9.4ms\n",
      "Speed: 1.7ms preprocess, 9.4ms inference, 32.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 40 Chicks, 8.9ms\n",
      "Speed: 1.4ms preprocess, 8.9ms inference, 36.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 34 Chicks, 9.1ms\n",
      "Speed: 1.5ms preprocess, 9.1ms inference, 35.8ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 35 Chicks, 8.7ms\n",
      "Speed: 1.4ms preprocess, 8.7ms inference, 36.8ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 24 Chicks, 9.4ms\n",
      "Speed: 1.5ms preprocess, 9.4ms inference, 22.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 27 Chicks, 9.3ms\n",
      "Speed: 1.4ms preprocess, 9.3ms inference, 25.8ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 29 Chicks, 9.0ms\n",
      "Speed: 1.5ms preprocess, 9.0ms inference, 57.8ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 22 Chicks, 8.9ms\n",
      "Speed: 1.6ms preprocess, 8.9ms inference, 20.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 22 Chicks, 9.1ms\n",
      "Speed: 1.4ms preprocess, 9.1ms inference, 19.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 25 Chicks, 9.3ms\n",
      "Speed: 1.4ms preprocess, 9.3ms inference, 25.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 30 Chicks, 8.9ms\n",
      "Speed: 1.5ms preprocess, 8.9ms inference, 32.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 33 Chicks, 9.5ms\n",
      "Speed: 1.5ms preprocess, 9.5ms inference, 39.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 37 Chicks, 10.0ms\n",
      "Speed: 1.5ms preprocess, 10.0ms inference, 36.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 32 Chicks, 9.1ms\n",
      "Speed: 1.6ms preprocess, 9.1ms inference, 32.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 28 Chicks, 10.2ms\n",
      "Speed: 1.6ms preprocess, 10.2ms inference, 28.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 31 Chicks, 10.5ms\n",
      "Speed: 1.8ms preprocess, 10.5ms inference, 31.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 33 Chicks, 10.1ms\n",
      "Speed: 1.4ms preprocess, 10.1ms inference, 36.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 24 Chicks, 9.5ms\n",
      "Speed: 1.6ms preprocess, 9.5ms inference, 25.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 38 Chicks, 10.4ms\n",
      "Speed: 1.4ms preprocess, 10.4ms inference, 39.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 28 Chicks, 10.5ms\n",
      "Speed: 1.5ms preprocess, 10.5ms inference, 48.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 27 Chicks, 11.0ms\n",
      "Speed: 1.6ms preprocess, 11.0ms inference, 42.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 31 Chicks, 9.2ms\n",
      "Speed: 1.9ms preprocess, 9.2ms inference, 36.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 30 Chicks, 10.3ms\n",
      "Speed: 1.4ms preprocess, 10.3ms inference, 34.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 27 Chicks, 10.9ms\n",
      "Speed: 1.7ms preprocess, 10.9ms inference, 28.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 38 Chicks, 10.0ms\n",
      "Speed: 1.5ms preprocess, 10.0ms inference, 44.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 39 Chicks, 10.5ms\n",
      "Speed: 1.4ms preprocess, 10.5ms inference, 41.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 35 Chicks, 31.9ms\n",
      "Speed: 2.1ms preprocess, 31.9ms inference, 86.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 37 Chicks, 18.9ms\n",
      "Speed: 1.5ms preprocess, 18.9ms inference, 39.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 35 Chicks, 11.3ms\n",
      "Speed: 1.4ms preprocess, 11.3ms inference, 44.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 36 Chicks, 9.9ms\n",
      "Speed: 1.8ms preprocess, 9.9ms inference, 55.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 35 Chicks, 12.7ms\n",
      "Speed: 1.5ms preprocess, 12.7ms inference, 61.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 36 Chicks, 14.5ms\n",
      "Speed: 1.9ms preprocess, 14.5ms inference, 42.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 28 Chicks, 11.5ms\n",
      "Speed: 1.5ms preprocess, 11.5ms inference, 35.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 29 Chicks, 7.8ms\n",
      "Speed: 1.4ms preprocess, 7.8ms inference, 38.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 39 Chicks, 8.4ms\n",
      "Speed: 1.5ms preprocess, 8.4ms inference, 34.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 32 Chicks, 8.8ms\n",
      "Speed: 2.1ms preprocess, 8.8ms inference, 31.8ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 34 Chicks, 8.5ms\n",
      "Speed: 1.4ms preprocess, 8.5ms inference, 31.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 35 Chicks, 8.4ms\n",
      "Speed: 1.4ms preprocess, 8.4ms inference, 33.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 30 Chicks, 8.3ms\n",
      "Speed: 1.4ms preprocess, 8.3ms inference, 30.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 37 Chicks, 7.8ms\n",
      "Speed: 1.4ms preprocess, 7.8ms inference, 32.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 41 Chicks, 8.0ms\n",
      "Speed: 1.4ms preprocess, 8.0ms inference, 40.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 46 Chicks, 7.9ms\n",
      "Speed: 1.6ms preprocess, 7.9ms inference, 46.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 30 Chicks, 9.0ms\n",
      "Speed: 1.6ms preprocess, 9.0ms inference, 54.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 28 Chicks, 9.2ms\n",
      "Speed: 1.3ms preprocess, 9.2ms inference, 35.8ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 29 Chicks, 10.6ms\n",
      "Speed: 1.4ms preprocess, 10.6ms inference, 30.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 33 Chicks, 8.2ms\n",
      "Speed: 1.6ms preprocess, 8.2ms inference, 71.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 26 Chicks, 16.8ms\n",
      "Speed: 1.8ms preprocess, 16.8ms inference, 24.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 29 Chicks, 9.4ms\n",
      "Speed: 1.6ms preprocess, 9.4ms inference, 26.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 25 Chicks, 9.5ms\n",
      "Speed: 1.4ms preprocess, 9.5ms inference, 32.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 20 Chicks, 9.5ms\n",
      "Speed: 1.4ms preprocess, 9.5ms inference, 19.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 20 Chicks, 12.2ms\n",
      "Speed: 2.1ms preprocess, 12.2ms inference, 23.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 28 Chicks, 8.9ms\n",
      "Speed: 1.4ms preprocess, 8.9ms inference, 37.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 23 Chicks, 11.4ms\n",
      "Speed: 1.5ms preprocess, 11.4ms inference, 22.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 30 Chicks, 11.2ms\n",
      "Speed: 1.7ms preprocess, 11.2ms inference, 32.8ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 37 Chicks, 11.6ms\n",
      "Speed: 1.8ms preprocess, 11.6ms inference, 44.8ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 26 Chicks, 9.5ms\n",
      "Speed: 2.2ms preprocess, 9.5ms inference, 28.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 20 Chicks, 9.1ms\n",
      "Speed: 1.6ms preprocess, 9.1ms inference, 32.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 28 Chicks, 8.9ms\n",
      "Speed: 1.4ms preprocess, 8.9ms inference, 25.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 32 Chicks, 9.0ms\n",
      "Speed: 1.4ms preprocess, 9.0ms inference, 33.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 24 Chicks, 9.0ms\n",
      "Speed: 1.5ms preprocess, 9.0ms inference, 24.8ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 29 Chicks, 13.0ms\n",
      "Speed: 1.7ms preprocess, 13.0ms inference, 38.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 41 Chicks, 9.6ms\n",
      "Speed: 1.5ms preprocess, 9.6ms inference, 41.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 47 Chicks, 9.0ms\n",
      "Speed: 1.6ms preprocess, 9.0ms inference, 50.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 43 Chicks, 10.1ms\n",
      "Speed: 1.6ms preprocess, 10.1ms inference, 38.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 41 Chicks, 9.4ms\n",
      "Speed: 1.8ms preprocess, 9.4ms inference, 42.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Total Frames Processed: 417\n",
      "YOLO Count = 83, BBV Total = 78, True Count = 75\n"
     ]
    }
   ],
   "source": [
    "def get_line_from_video_frame(frame):\n",
    "    frame_height, frame_width = frame.shape[:2]\n",
    "    # Draw a horizontal line across the middle of the frame\n",
    "    line_start = (frame_width, frame_height // 2)\n",
    "    line_end = (0, frame_height // 2)\n",
    "    return [line_start, line_end]\n",
    "\n",
    "MIN_BOX_AREA = 900  # Minimum area of bounding box to consider for BBV\n",
    "\n",
    "def chick_counting(video_path, output_path, line_points, verbose = False):\n",
    "\n",
    "    # Grab a sample frame so we know video size\n",
    "    generator = sv.get_video_frames_generator(video_path)\n",
    "    frame = next(generator)\n",
    "\n",
    "    # Set up video writer with same FPS/size as input\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    cap.release()\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (frame.shape[1], frame.shape[0]))\n",
    "    if not out.isOpened():\n",
    "        print(\"Error: Could not open video writer\")\n",
    "        return\n",
    "\n",
    "    # Init tracker and helpers\n",
    "    byte_tracker = sv.ByteTrack()\n",
    "    trace_annotator = sv.TraceAnnotator(thickness=4, trace_length=50)\n",
    "\n",
    "    # Create the counting line\n",
    "    line_zone = sv.LineZone(start=sv.Point(*line_points[0]), end=sv.Point(*line_points[1]))\n",
    "\n",
    "    # Load custom YOLO model\n",
    "    model = YOLO_MODEL\n",
    "    \n",
    "    # Annotators for boxes + labels\n",
    "    BOUNDING_BOX_ANNOTATOR = sv.BoxAnnotator(thickness=2, color=sv.Color(0, 255, 0))\n",
    "    LABEL_ANNOTATOR = sv.LabelAnnotator(text_scale=1)\n",
    "\n",
    "    # Counters\n",
    "    frame_count = 0\n",
    "    total_count = 0\n",
    "    total_count_bbv = 0\n",
    "    all_counted_ids = set()  # keep track of already-counted trackers\n",
    "    all_counted_ids_bbv = set()  # Seperate list for the bounding box validation\n",
    "    \n",
    "    # Constants to hold high and low thermal temperatures for denormalization\n",
    "    prev_hi = None\n",
    "    prev_lo = None\n",
    "\n",
    "    try:\n",
    "        generator = sv.get_video_frames_generator(video_path)\n",
    "\n",
    "        for frame in generator:\n",
    "            frame_count += 1\n",
    "            if verbose:\n",
    "                print(f\"Processing frame {frame_count}\")\n",
    "\n",
    "            # Run YOLO on frame\n",
    "            #results = model(frame)[0]\n",
    "            results = model(frame, conf=0.10, iou=0.9)[0]  # Slight tweaks - decrease confidence from .25->.15, increase iou from .7->.75. Make it less strict\n",
    "            \n",
    "            # Get the frame image as denormalized numpy array\n",
    "            try:\n",
    "                temp_arr, prev_hi, prev_lo = result_to_temp_frame(\n",
    "                    results,\n",
    "                    frame_idx = frame_count,\n",
    "                    prev_hi_val = prev_hi,\n",
    "                    prev_lo_val = prev_lo\n",
    "                )\n",
    "            except Exception as e:\n",
    "                if verbose:\n",
    "                    print(f\"Warning: Could not convert frame {frame_count} to temperature array. Skipping BBV for this frame.\")\n",
    "                if prev_hi is not None and prev_lo is not None:\n",
    "                    temp_arr = np.zeros_like(frame[..., 0], dtype=np.float32)  # reuse last temp_arr shape\n",
    "                else:\n",
    "                    temp_arr = None\n",
    "\n",
    "            # Convert results to supervision Detections\n",
    "            detections = sv.Detections.from_ultralytics(results)\n",
    "\n",
    "            # Sensitivity for declaring a box as \"nested\" (e.g. 0.9 means inner must have at least 90% of its area inside outer)\n",
    "            NESTED_THRESHOLD = 0.9  \n",
    "\n",
    "            # Get indicies of all boxes\n",
    "            contained_indices = set()\n",
    "            boxes = detections.xyxy\n",
    "\n",
    "            for i, outer in enumerate(boxes):\n",
    "                x1o, y1o, x2o, y2o = outer\n",
    "                outer_area = max(0, (x2o - x1o)) * max(0, (y2o - y1o))\n",
    "\n",
    "                for j, inner in enumerate(boxes):\n",
    "                    if i == j:\n",
    "                        continue\n",
    "                    x1i, y1i, x2i, y2i = inner\n",
    "                    inner_area = max(0, (x2i - x1i)) * max(0, (y2i - y1i))\n",
    "\n",
    "                    # Intersection box\n",
    "                    inter_x1 = max(x1o, x1i)\n",
    "                    inter_y1 = max(y1o, y1i)\n",
    "                    inter_x2 = min(x2o, x2i)\n",
    "                    inter_y2 = min(y2o, y2i)\n",
    "\n",
    "                    inter_area = max(0, inter_x2 - inter_x1) * max(0, inter_y2 - inter_y1)\n",
    "\n",
    "                    # Ratio of inner covered by outer\n",
    "                    if inner_area > 0 and (inter_area / inner_area) >= NESTED_THRESHOLD:\n",
    "                        contained_indices.add(j)\n",
    "\n",
    "\n",
    "            # Update tracker with detections\n",
    "            detections = byte_tracker.update_with_detections(detections)\n",
    "            if verbose:\n",
    "                print(\"Tracker IDs this frame:\", detections.tracker_id)\n",
    "\n",
    "            # See if any trackers crossed the line\n",
    "            crossed_in_flags, crossed_out_flags = line_zone.trigger(detections)\n",
    "\n",
    "            # Only count new IDs that cross \"in\"\n",
    "            for i, crossed in enumerate(crossed_in_flags):\n",
    "                if crossed:\n",
    "                    tracker_id = detections.tracker_id[i]\n",
    "                    \n",
    "                    if tracker_id is None: continue  # Skip if no tracker ID\n",
    "                    \n",
    "                    # Track bounding boxes\n",
    "                    if tracker_id not in all_counted_ids:\n",
    "                        # Add to the YOLO total count\n",
    "                        total_count += 1\n",
    "                        all_counted_ids.add(tracker_id)\n",
    "                        \n",
    "                        # TESTING - Determine area to filter out unreasonably small boxes\n",
    "                        x1, y1, x2, y2 = map(int, detections.xyxy[i])\n",
    "                        w = max(0, x2 - x1)\n",
    "                        h = max(0, y2 - y1)\n",
    "                        area = w * h\n",
    "                            \n",
    "                        # Run the BBV on this bouning box\n",
    "                        if area > MIN_BOX_AREA:\n",
    "                            # cur_box_count = get_box_count(\n",
    "                            #     pipeline=BBV_PIPELINE_NO_GROUPING,\n",
    "                            #     temperature_frame=temp_arr,\n",
    "                            #     box=(x1, y1, x2, y2)\n",
    "                            # )\n",
    "                            # TESTING ONLY\n",
    "                            cur_box_count = 1\n",
    "                            total_count_bbv += cur_box_count\n",
    "                        \n",
    "                        if verbose:\n",
    "                            print(f\"New Chick crossed the line! ID {tracker_id}, Chicks in bounding box: {cur_box_count}, Total YOLO count: {total_count}, Total BBV count: {total_count_bbv}\")\n",
    "\n",
    "            # Assign labels + colors depending on nesting\n",
    "            labels = []\n",
    "            colors = []\n",
    "            for i, tracker_id in enumerate(detections.tracker_id):\n",
    "                if i in contained_indices:\n",
    "                    labels.append(f\"#{tracker_id} nested\")\n",
    "                    colors.append(sv.Color.RED)\n",
    "                else:\n",
    "                    labels.append(f\"#{tracker_id} chick\")\n",
    "                    colors.append(sv.Color.GREEN)\n",
    "\n",
    "            # Draw tracker trails\n",
    "            annotated_frame = trace_annotator.annotate(scene=frame.copy(), detections=detections)\n",
    "\n",
    "            # Draw bounding boxes manually with chosen colors\n",
    "            for i, box in enumerate(detections.xyxy):\n",
    "                color = colors[i] if i < len(colors) else sv.Color.GREEN\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color.as_bgr(), 2)\n",
    "\n",
    "            # Draw labels\n",
    "            # Draw smaller labels with smaller background rectangles\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            font_scale = 0.4  # significantly smaller\n",
    "            thickness = 1\n",
    "            pad = 3\n",
    "\n",
    "            for i, lbl in enumerate(labels):\n",
    "                x1, y1, x2, y2 = map(int, detections.xyxy[i])\n",
    "                width = max(0, x2 - x1)\n",
    "                height = max(0, y2 - y1)\n",
    "                text_size, _ = cv2.getTextSize(lbl, font, font_scale, thickness)\n",
    "                text_w, text_h = text_size\n",
    "\n",
    "                # Position label above box if space, otherwise below\n",
    "                if y1 - text_h - 2 * pad > 0:\n",
    "                    rect_tl = (x1, y1 - text_h - 2 * pad)\n",
    "                    rect_br = (x1 + text_w + 2 * pad, y1)\n",
    "                    text_org = (x1 + pad, y1 - pad)\n",
    "                else:\n",
    "                    rect_tl = (x1, y1)\n",
    "                    rect_br = (x1 + text_w + 2 * pad, y1 + text_h + 2 * pad)\n",
    "                    text_org = (x1 + pad, y1 + text_h + pad)\n",
    "\n",
    "                # Background color: use tracker color if available, else black\n",
    "                bg_color = colors[i].as_bgr() if i < len(colors) else (0, 0, 0)\n",
    "                # Choose text color for contrast\n",
    "                text_color = (0, 0, 0) if sum(bg_color) > 382 else (255, 255, 255)\n",
    "\n",
    "                cv2.rectangle(annotated_frame, rect_tl, rect_br, bg_color, cv2.FILLED)\n",
    "                cv2.putText(annotated_frame, lbl, text_org, font, font_scale, text_color, thickness, cv2.LINE_AA)\n",
    "\n",
    "            # Draw the counting line\n",
    "            cv2.line(annotated_frame, line_points[0], line_points[1], (0, 0, 255), 2)\n",
    "\n",
    "            # Overlay YOLO total count\n",
    "            cv2.putText(\n",
    "                annotated_frame,\n",
    "                f'Total Count: {total_count}',\n",
    "                (10, 50),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.5,\n",
    "                (0, 255, 0),\n",
    "                2,\n",
    "                cv2.LINE_AA\n",
    "            )\n",
    "            \n",
    "            # Overlay BBV total count\n",
    "            cv2.putText(\n",
    "                annotated_frame,\n",
    "                f'BBV Total Count: {total_count_bbv}',\n",
    "                (10, 80),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.5,\n",
    "                (255, 0, 0),\n",
    "                2,\n",
    "                cv2.LINE_AA\n",
    "            )\n",
    "            \n",
    "            # Overlay True total count\n",
    "            cv2.putText(\n",
    "                annotated_frame,\n",
    "                f'True Total Count: {TRUE_COUNT}',\n",
    "                (10, 110),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.5,\n",
    "                (0, 0, 255),\n",
    "                2,\n",
    "                cv2.LINE_AA\n",
    "            )\n",
    "\n",
    "            # Write out annotated frame\n",
    "            out.write(annotated_frame)\n",
    "            \n",
    "    except Exception as e:\n",
    "        # Detailed exception logging\n",
    "        print(\"=== Exception while processing video frames ===\")\n",
    "        print(\"Time:\", datetime.datetime.now().isoformat())\n",
    "        print(\"Exception type:\", type(e).__name__)\n",
    "        print(\"Exception message:\", str(e))\n",
    "        print(\"Full traceback:\")\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "    finally:\n",
    "        # Clean up writer and windows\n",
    "        out.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        print(f\"Total Frames Processed: {frame_count}\\nYOLO Count = {total_count}, BBV Total = {total_count_bbv}, True Count = {TRUE_COUNT}\")\n",
    "        if verbose:\n",
    "            print(f\"LineZone internal count (for reference): in={line_zone.in_count}, out={line_zone.out_count}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import tkinter as tk\n",
    "    from tkinter.filedialog import askopenfilename, askdirectory\n",
    "    tk.Tk().withdraw()\n",
    "\n",
    "    # Pick input video + output folder with file dialogs\n",
    "    SOURCE_VIDEO_PATH = askopenfilename()\n",
    "    print(\"User chose:\", SOURCE_VIDEO_PATH)\n",
    "\n",
    "    folder_path = askdirectory()\n",
    "    print(\"Output folder:\", folder_path)\n",
    "\n",
    "    # Build output filename\n",
    "    filename_no_ext = SOURCE_VIDEO_PATH.split('/')[-1].rsplit('.', 1)[0]\n",
    "    OUTPUT_PATH = f\"{folder_path}/{filename_no_ext}-PIPELINETEST-outputfile(colored).mp4\"\n",
    "    print(\"Output path:\", OUTPUT_PATH)\n",
    "\n",
    "    # Grab a frame to define the line\n",
    "    cap = cv2.VideoCapture(SOURCE_VIDEO_PATH)\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to read the video\")\n",
    "        exit()\n",
    "    cap.release()\n",
    "\n",
    "    line_points = get_line_from_video_frame(frame)\n",
    "    print(\"Line points:\", line_points)\n",
    "\n",
    "    # Only run if line points are valid\n",
    "    if len(line_points) == 2:\n",
    "        chick_counting(SOURCE_VIDEO_PATH, OUTPUT_PATH, line_points, verbose=False)\n",
    "    else:\n",
    "        print(\"Error: Not enough points to define the counting line.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
