{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d1b6b2b",
   "metadata": {},
   "source": [
    "# Thermal Counting Training\n",
    "\n",
    "Rough training pipeline for gathering and labeling grouped bounding boxes.\n",
    "\n",
    "> Note: If there is a tkinter error, run ```sudo apt install python3-tk -y```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae045d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'validate_bounding_box' from '/mnt/c/Users/JJ/Desktop/Repos/Chick-Counting/thermal/validate_bounding_box.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib  # Refreshing imports\n",
    "\n",
    "# Core libraries\n",
    "import cv2\n",
    "import supervision as sv\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from utils.thermal_frame_to_temp import result_to_temp_frame\n",
    "import utils.group_bounding_boxes as gbb\n",
    "import validate_bounding_box as vbb\n",
    "import tkinter as tk\n",
    "from tkinter.filedialog import askopenfilename, askdirectory\n",
    "import joblib\n",
    "# SVM and model training\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Better exception handling and helpers\n",
    "import traceback\n",
    "import pprint\n",
    "import datetime\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Refresh imports to see live changes vs cached ones\n",
    "importlib.reload(gbb)\n",
    "importlib.reload(vbb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e7b8026",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Declaraing the save directory for grouped bounding box crops '''\n",
    "SAVE_DIR = \"grouped_bounding_box_crops\"  # Directory to save the crops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fd58a3",
   "metadata": {},
   "source": [
    "## Helpers for saving the results (cropped bounding box results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "766e4412",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_crop(frame: np.ndarray, box: tuple[int,int,int,int], frame_count: int, group_id: int) -> None:\n",
    "    \"\"\"Safely crop and save each combined bounding box to SAVE_DIR with a unique name.\"\"\"\n",
    "    \n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    h, w = frame.shape[:2]\n",
    "    x1, y1, x2, y2 = box\n",
    "    \n",
    "    # Clip to frame\n",
    "    x1 = max(0, min(x1, w-1)); x2 = max(0, min(x2, w-1))\n",
    "    y1 = max(0, min(y1, h-1)); y2 = max(0, min(y2, h-1))\n",
    "    \n",
    "    # Validate the coordinates\n",
    "    if x2 <= x1 or y2 <= y1:\n",
    "        return\n",
    "    \n",
    "    # Perform the crop and save\n",
    "    crop = frame[y1:y2, x1:x2].copy()\n",
    "    ts = int(time.time() * 1000)\n",
    "    out_path = os.path.join(SAVE_DIR, f\"f{frame_count}_g{group_id}_{ts}.jpg\")\n",
    "    cv2.imwrite(out_path, crop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a4a68d",
   "metadata": {},
   "source": [
    "## Run the YOLO model and gather the results (currently unoptimized, POC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ecadafb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User chose: ()\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.12.0) :-1: error: (-5:Bad argument) in function 'VideoCapture'\n> Overload resolution failed:\n>  - Expected 'filename' to be a str or path-like object\n>  - VideoCapture() missing required argument 'apiPreference' (pos 2)\n>  - Argument 'index' is required to be an integer\n>  - VideoCapture() missing required argument 'apiPreference' (pos 2)\n>  - VideoCapture() missing required argument 'apiPreference' (pos 2)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31merror\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 154\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mUser chose:\u001b[39m\u001b[33m\"\u001b[39m, SOURCE_VIDEO_PATH)\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m# Grab a frame to define the line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m cap = \u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mVideoCapture\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSOURCE_VIDEO_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m ret, frame = cap.read()\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n",
      "\u001b[31merror\u001b[39m: OpenCV(4.12.0) :-1: error: (-5:Bad argument) in function 'VideoCapture'\n> Overload resolution failed:\n>  - Expected 'filename' to be a str or path-like object\n>  - VideoCapture() missing required argument 'apiPreference' (pos 2)\n>  - Argument 'index' is required to be an integer\n>  - VideoCapture() missing required argument 'apiPreference' (pos 2)\n>  - VideoCapture() missing required argument 'apiPreference' (pos 2)\n"
     ]
    }
   ],
   "source": [
    "FRAME_COUNT_EARLY_STOP = 1000  # For testing, limit to first N frames\n",
    "\n",
    "def get_line_from_video_frame(frame):\n",
    "    frame_height, frame_width = frame.shape[:2]\n",
    "\n",
    "    # Draw a horizontal line across the middle of the frame\n",
    "    line_start = (frame_width, frame_height // 2)\n",
    "    line_end = (0, frame_height // 2)\n",
    "    return [line_start, line_end]\n",
    "\n",
    "def chick_counting(video_path, line_points):\n",
    "\n",
    "    # Grab a sample frame so we know video size\n",
    "    generator = sv.get_video_frames_generator(video_path)\n",
    "    frame = next(generator)\n",
    "\n",
    "    # Set up video writer with same FPS/size as input\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    cap.release()\n",
    "\n",
    "    # Init tracker and helpers\n",
    "    byte_tracker = sv.ByteTrack()\n",
    "    trace_annotator = sv.TraceAnnotator(thickness=4, trace_length=50)\n",
    "\n",
    "    # Create the counting line\n",
    "    line_zone = sv.LineZone(start=sv.Point(*line_points[0]), end=sv.Point(*line_points[1]))\n",
    "\n",
    "    # Load custom YOLO model (trained on chicks only)\n",
    "    model = YOLO(\"models/new_iron.pt\")\n",
    "\n",
    "    frame_count = 0\n",
    "    total_count = 0\n",
    "    all_counted_ids = set()  # keep track of already-counted trackers\n",
    "\n",
    "    try:\n",
    "        generator = sv.get_video_frames_generator(video_path)\n",
    "\n",
    "        for frame in generator:\n",
    "            frame_count += 1\n",
    "            if frame_count > FRAME_COUNT_EARLY_STOP:\n",
    "                break\n",
    "            \n",
    "            print(f\"Processing frame {frame_count}\")\n",
    "\n",
    "            # Run YOLO on frame\n",
    "            results = model(frame)[0]\n",
    "\n",
    "            # Convert results to supervision Detections\n",
    "            detections = sv.Detections.from_ultralytics(results)\n",
    "\n",
    "            # Update tracker with detections\n",
    "            detections = byte_tracker.update_with_detections(detections)\n",
    "            print(\"Tracker IDs this frame:\", detections.tracker_id)\n",
    "\n",
    "            # See if any trackers crossed the line\n",
    "            crossed_in_flags, crossed_out_flags = line_zone.trigger(detections)\n",
    "\n",
    "            ''' Additional training logic for grouping bounding boxes and saving crops '''\n",
    "            # Find groups that contain any box overlapping the detection that crossed \"in\"\n",
    "            xyxy_np = detections.xyxy.astype(float)\n",
    "            groups = gbb.group_bounding_boxes(xyxy_np)  # Default, low threshold for now\n",
    "            \n",
    "            # Finding which groups correspond to crossed \"in\" boxes\n",
    "            crossed_i = {i for i, crossed in enumerate(crossed_in_flags) if crossed}\n",
    "            groups_to_save = []\n",
    "            for gid, g in enumerate(groups):\n",
    "                if any(idx in crossed_i for idx in g):\n",
    "                    groups_to_save.append((gid, g))\n",
    "                    \n",
    "            # Merge and save the grouped boxes for each group\n",
    "            for gid, g in groups_to_save:\n",
    "                # Skip empty groups\n",
    "                if not g:\n",
    "                    continue\n",
    "                # Get merged box and save crop\n",
    "                merged_box = gbb.merge_group_bounding_box(xyxy_np, g)\n",
    "                save_crop(frame, merged_box, frame_count, gid)\n",
    "            \n",
    "            # Only count new IDs that cross \"in\"\n",
    "            for i, crossed in enumerate(crossed_in_flags):\n",
    "                if crossed:\n",
    "                    tracker_id = detections.tracker_id[i]\n",
    "                    if tracker_id is not None and tracker_id not in all_counted_ids:\n",
    "                        total_count += 1\n",
    "                        all_counted_ids.add(tracker_id)\n",
    "                        print(f\"New Chick crossed the line! ID {tracker_id}, Total count: {total_count}\")\n",
    "\n",
    "            # Sensitivity for declaring a box as \"nested\"\n",
    "            # e.g. 0.9 means inner must have at least 90% of its area inside outer\n",
    "            NESTED_THRESHOLD = 0.9  \n",
    "\n",
    "            contained_indices = set()\n",
    "            boxes = detections.xyxy\n",
    "\n",
    "            for i, outer in enumerate(boxes):\n",
    "                x1o, y1o, x2o, y2o = outer\n",
    "                outer_area = max(0, (x2o - x1o)) * max(0, (y2o - y1o))\n",
    "\n",
    "                for j, inner in enumerate(boxes):\n",
    "                    if i == j:\n",
    "                        continue\n",
    "                    x1i, y1i, x2i, y2i = inner\n",
    "                    inner_area = max(0, (x2i - x1i)) * max(0, (y2i - y1i))\n",
    "\n",
    "                    # Intersection box\n",
    "                    inter_x1 = max(x1o, x1i)\n",
    "                    inter_y1 = max(y1o, y1i)\n",
    "                    inter_x2 = min(x2o, x2i)\n",
    "                    inter_y2 = min(y2o, y2i)\n",
    "\n",
    "                    inter_area = max(0, inter_x2 - inter_x1) * max(0, inter_y2 - inter_y1)\n",
    "\n",
    "                    # Ratio of inner covered by outer\n",
    "                    if inner_area > 0 and (inter_area / inner_area) >= NESTED_THRESHOLD:\n",
    "                        contained_indices.add(j)\n",
    "\n",
    "\n",
    "            # Assign labels + colors depending on nesting\n",
    "            labels = []\n",
    "            colors = []\n",
    "            for i, tracker_id in enumerate(detections.tracker_id):\n",
    "                if i in contained_indices:\n",
    "                    labels.append(f\"#{tracker_id} nested\")\n",
    "                    colors.append(sv.Color.RED)\n",
    "                else:\n",
    "                    labels.append(f\"#{tracker_id} chick\")\n",
    "                    colors.append(sv.Color.GREEN)\n",
    "\n",
    "            \n",
    "\n",
    "    except Exception as e:\n",
    "        # Detailed exception logging\n",
    "        print(\"=== Exception while processing video frames ===\")\n",
    "        print(\"Time:\", datetime.datetime.now().isoformat())\n",
    "        print(\"Exception type:\", type(e).__name__)\n",
    "        print(\"Exception message:\", str(e))\n",
    "        print(\"Full traceback:\")\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "    finally:\n",
    "        print(f\"Processing complete. Processed {frame_count} frames.\")\n",
    "        print(f\"Final total count: {total_count}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tk.Tk().withdraw()\n",
    "\n",
    "    # Pick input video + output folder with file dialogs\n",
    "    SOURCE_VIDEO_PATH = askopenfilename()\n",
    "    print(\"User chose:\", SOURCE_VIDEO_PATH)\n",
    "    \n",
    "    # Grab a frame to define the line\n",
    "    cap = cv2.VideoCapture(SOURCE_VIDEO_PATH)\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to read the video\")\n",
    "        exit()\n",
    "    cap.release()\n",
    "    \n",
    "    line_points = get_line_from_video_frame(frame)\n",
    "    \n",
    "    chick_counting(SOURCE_VIDEO_PATH, line_points)\n",
    "    \n",
    "    print(f\"Completed attempted processing of {FRAME_COUNT_EARLY_STOP} frames.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b84d11",
   "metadata": {},
   "source": [
    "## Extracting features from collected (and hand-labeled) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7616871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved extracted features for chick counting model training.\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "categories: List[int] = [0, 1, 2, 3, 4]  # Categories for different chick counts\n",
    "data: List[np.array] = []  # Will hold the extracted feature data\n",
    "\n",
    "# Loop through each category directory and extract features\n",
    "for category in categories:\n",
    "    path = os.path.join(SAVE_DIR, f\"{str(category)}\")\n",
    "    # Each image in the category directory\n",
    "    for img in os.listdir(path):\n",
    "        # Load the image into CV2\n",
    "        img_path = os.path.join(path, img)\n",
    "        image = cv2.imread(img_path)\n",
    "        # Skip if image failed to load\n",
    "        if image is None:\n",
    "            continue\n",
    "        \n",
    "        # NOTE Blocker - How can we get the temperature data at this point?\n",
    "        \n",
    "        # Temporary Solution - Plain Normalization\n",
    "        image_temp = (image - np.min(image)) / (np.max(image) - np.min(image))\n",
    "        image_temp = image_temp.astype(np.float32)\n",
    "        \n",
    "        # Extract features using validate_bounding_box module\n",
    "        image_features = vbb.get_box_features(image_temp)\n",
    "        \n",
    "        data.append((category, image_features))\n",
    "        \n",
    "# Save the extracted data for later model training\n",
    "joblib.dump(data, \"thermal_chick_counting_features.pkl\")\n",
    "print(\"Saved extracted features for chick counting model training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76c1f12",
   "metadata": {},
   "source": [
    "## Training/Testing a basic SVM Model\n",
    "\n",
    "Running a GridSearchCV to find the best cross-validated result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb925e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded features for model training: 152 samples.\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "[CV] END ........................................svm__C=0.01; total time=   0.0s\n",
      "[CV] END ........................................svm__C=0.01; total time=   0.0s\n",
      "[CV] END ........................................svm__C=0.01; total time=   0.0s\n",
      "[CV] END .........................................svm__C=0.1; total time=   0.0s\n",
      "[CV] END .........................................svm__C=0.1; total time=   0.0s\n",
      "[CV] END .........................................svm__C=0.1; total time=   0.0s[CV] END .........................................svm__C=1.0; total time=   0.0s\n",
      "\n",
      "[CV] END .........................................svm__C=1.0; total time=   0.0s\n",
      "[CV] END ........................................svm__C=10.0; total time=   0.0s\n",
      "[CV] END .........................................svm__C=1.0; total time=   0.0s\n",
      "[CV] END .......................................svm__C=100.0; total time=   0.0s\n",
      "[CV] END ........................................svm__C=10.0; total time=   0.0s\n",
      "[CV] END ........................................svm__C=10.0; total time=   0.0s\n",
      "[CV] END .......................................svm__C=100.0; total time=   0.0s\n",
      "[CV] END .......................................svm__C=100.0; total time=   0.0s\n",
      "Best params: {'svm__C': 10.0}\n",
      "Best cross-validation accuracy: 0.7962091503267974\n",
      "0.01: mean CV accuracy = 0.7698\n",
      "0.1: mean CV accuracy = 0.7698\n",
      "1.0: mean CV accuracy = 0.7698\n",
      "10.0: mean CV accuracy = 0.7962\n",
      "100.0: mean CV accuracy = 0.7830\n",
      "Re-evaluated CV scores for best estimator: [    0.80392     0.76471        0.82]\n",
      "Re-evaluated mean accuracy: 0.7962091503267974\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Loading in the features (if not already loaded)\n",
    "features = joblib.load(\"thermal_chick_counting_features.pkl\")\n",
    "\n",
    "print(\"Loaded features for model training:\", len(features), \"samples.\")\n",
    "\n",
    "# Unpack into targets and features\n",
    "y = np.array([label for label, _ in features], dtype=int)\n",
    "X = np.array([feat for _, feat in features], dtype=np.float32)\n",
    "\n",
    "# Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', SVC(kernel='rbf', gamma='scale', probability=True,\n",
    "                class_weight={0:0.00, 1:0.77, 2:0.171, 3:0.039, 4:0.02}))\n",
    "])\n",
    "\n",
    "# Stratified K-Fold for cross-validation\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Parameter grid to test different C values\n",
    "param_grid = {\n",
    "    'svm__C': [0.01, 0.1, 1.0, 10.0, 100.0]  # Tweaked for optimal range\n",
    "}\n",
    "\n",
    "# Grid search using the same StratifiedKFold\n",
    "grid = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=skf,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    return_train_score=False\n",
    ")\n",
    "\n",
    "grid.fit(X, y)\n",
    "\n",
    "# Print the best parameter set\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "print(\"Best cross-validation accuracy:\", grid.best_score_)\n",
    "\n",
    "# Show mean score per tested C\n",
    "for mean, params in zip(grid.cv_results_['mean_test_score'], grid.cv_results_['params']):\n",
    "    print(f\"{params['svm__C']}: mean CV accuracy = {mean:.4f}\")\n",
    "\n",
    "# Evaluate best estimator with cross_val_score to confirm\n",
    "best_est = grid.best_estimator_\n",
    "best_scores = cross_val_score(best_est, X, y, cv=skf, scoring='accuracy', n_jobs=-1)\n",
    "print(\"Re-evaluated CV scores for best estimator:\", best_scores)\n",
    "print(\"Re-evaluated mean accuracy:\", np.mean(best_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b2b9506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: Counter({np.int64(1): 117, np.int64(2): 26, np.int64(3): 6, np.int64(4): 3})\n",
      "Class ratios: {np.int64(1): '0.770', np.int64(2): '0.171', np.int64(3): '0.039', np.int64(4): '0.020'}\n"
     ]
    }
   ],
   "source": [
    "''' Showing class distributions '''\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "counts = Counter(y)\n",
    "n_total = len(y)\n",
    "\n",
    "print(\"Class counts:\", counts)\n",
    "print(\"Class ratios:\", {c: f\"{counts[c]/n_total:.3f}\" for c in counts})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d7b24bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thermal_chick_counting_svm_model.pkl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Saving the trained model '''\n",
    "joblib.dump(grid.best_estimator_, \"thermal_chick_counting_svm_model.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
