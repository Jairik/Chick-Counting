{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2167b0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "from ultralytics import solutions  # kept for compatibility\n",
    "\n",
    "# -----------------------------\n",
    "# numerically-safe activations\n",
    "# -----------------------------\n",
    "def safe_softplus(z):\n",
    "    z = np.asarray(z, dtype=np.float64)\n",
    "    return np.log1p(np.exp(-np.abs(z))) + np.maximum(z, 0.0)\n",
    "\n",
    "def safe_sigmoid(z):\n",
    "    z = np.asarray(z, dtype=np.float64)\n",
    "    out = np.empty_like(z, dtype=np.float64)\n",
    "    pos = z >= 0\n",
    "    out[pos]  = 1.0 / (1.0 + np.exp(-z[pos]))\n",
    "    e = np.exp(z[~pos])\n",
    "    out[~pos] = e / (1.0 + e)\n",
    "    return out\n",
    "\n",
    "# -----------------------------\n",
    "# geometry helpers (center-line crossing)\n",
    "# -----------------------------\n",
    "def _orient(a, b, c):\n",
    "    return (b[0]-a[0])*(c[1]-a[1]) - (b[1]-a[1])*(c[0]-a[0])\n",
    "\n",
    "def _line_side(p, a, b):\n",
    "    s = _orient(a, b, p)\n",
    "    return 0 if s == 0 else (1 if s > 0 else -1)\n",
    "\n",
    "def _segments_intersect(p1, p2, q1, q2):\n",
    "    o1 = _orient(p1, p2, q1)\n",
    "    o2 = _orient(p1, p2, q2)\n",
    "    o3 = _orient(q1, q2, p1)\n",
    "    o4 = _orient(q1, q2, p2)\n",
    "    if (o1 == 0 and o2 == 0 and o3 == 0 and o4 == 0):\n",
    "        return (min(p1[0], p2[0]) <= max(q1[0], q2[0]) and\n",
    "                min(q1[0], q2[0]) <= max(p1[0], p2[0]) and\n",
    "                min(p1[1], p2[1]) <= max(q1[1], q2[1]) and\n",
    "                min(q1[1], q2[1]) <= max(p1[1], p2[1]))\n",
    "    return (o1 == 0 or o2 == 0 or np.sign(o1) != np.sign(o2)) and \\\n",
    "           (o3 == 0 or o4 == 0 or np.sign(o3) != np.sign(o4))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# feature extractor (vector)\n",
    "#   - bbox area (normalized)\n",
    "#   - aspect ratio (w/h)\n",
    "#   - squareness = min(w,h)/max(w,h)\n",
    "#   - yellow metrics in HSV:\n",
    "#       * fraction of yellow pixels\n",
    "#       * mean S within yellow\n",
    "#       * mean V within yellow\n",
    "# -----------------------------\n",
    "import utils.get_bounding_box_features as bbf\n",
    "import utils.yellow_masks as ym\n",
    "def get_features(roi_bgr: np.ndarray) -> np.ndarray:\n",
    "    if roi_bgr.size == 0:\n",
    "        return np.zeros(6, dtype=np.float64)\n",
    "    h, w = roi_bgr.shape[:2]\n",
    "    area = float(h * w)\n",
    "    if area <= 1:\n",
    "        return np.zeros(6, dtype=np.float64)\n",
    "\n",
    "    # geometry\n",
    "    area_norm = area / 1e4                      # rough normalization scale\n",
    "    aspect = (w / max(h, 1.0))\n",
    "    square = min(w, h) / max(w, h)\n",
    "\n",
    "    # color (HSV)\n",
    "    hsv = cv2.cvtColor(roi_bgr, cv2.COLOR_BGR2HSV)\n",
    "    H, S, V = hsv[..., 0], hsv[..., 1], hsv[..., 2]\n",
    "\n",
    "    # yellow mask (OpenCV H in [0,179]): approx 20-40°, adjust if needed\n",
    "    yellow_lo, yellow_hi = 20, 40\n",
    "    S_lo, V_lo = 70, 100  # avoid very desaturated/dim pixels\n",
    "    ymask = (H >= yellow_lo) & (H <= yellow_hi) & (S >= S_lo) & (V >= V_lo)\n",
    "\n",
    "    y_count = float(np.count_nonzero(ymask))\n",
    "    y_frac  = y_count / area\n",
    "    if y_count > 0:\n",
    "        y_mean_s = float(S[ymask].mean()) / 255.0\n",
    "        y_mean_v = float(V[ymask].mean()) / 255.0\n",
    "    else:\n",
    "        y_mean_s = 0.0\n",
    "        y_mean_v = 0.0\n",
    "\n",
    "    # overall brightness can still help a bit\n",
    "    v_mean = float(V.mean()) / 255.0\n",
    "    \n",
    "    box = ym.get_yellow_mask(img=roi_bgr, normalize=True)\n",
    "    \n",
    "    area = bbf.get_area(box)  # Total area of the bounding box\n",
    "    mean_yellow = bbf.get_mean_yellow(box)  # Mean temperature in the bounding box\n",
    "    pthresh_o = bbf.get_pixels_over_threshold(box, 0.8)  # Standard deviation of temperatures\n",
    "    pthresh_u = bbf.get_pixels_under_threshold(box, 0.8)  # Variance of temperatures\n",
    "    y_range = bbf.get_yellow_range(box)  # Range of temperatures\n",
    "    y_std = bbf.get_yellow_std(box)  # Relative count of pixels over 30C\n",
    "    y_var = bbf.get_yellow_variance(box)  # Relative count of pixels under 30C\n",
    "    mdft = bbf.get_mean_distance_from_threshold(box, threshold=.8)  # Mean distance from 30C\n",
    "    asp_rat = bbf.get_aspect_ratio(box)  # Aspect ratio of the bounding box\n",
    "\n",
    "    return np.array([area_norm, aspect, square, y_frac, y_mean_s, y_mean_v, v_mean], dtype=np.float64)#, \\\n",
    "                     #area, mean_yellow, pthresh_o, pthresh_u, y_range, y_std, y_var, mdft, asp_rat], dtype=np.float64)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# vector head, softplus output, grouped loss, calibration\n",
    "# -------------------------------------------------------\n",
    "class GroupedSoftplusSGDVector:\n",
    "    \"\"\"\n",
    "    Event output y_i = softplus( w·x_i_std + b ) >= 0\n",
    "    Loss = Σ_k 0.5 * (Σ_{i∈Ik} y_i - dY_k)^2 + L2\n",
    "    After training, apply a non-negative scalar calibration α to match mass.\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=3e-3, epochs=800, l2=1e-5, clip=2.0, report_every=25, early_stop_patience=50):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.l2 = l2\n",
    "        self.clip = clip\n",
    "        self.report_every = report_every\n",
    "        self.early_stop_patience = early_stop_patience\n",
    "\n",
    "        self.w = None  # (D,)\n",
    "        self.b = 0.0\n",
    "        self.mu = None  # (D,)\n",
    "        self.sigma = None  # (D,)\n",
    "        self.alpha = 1.0  # final calibration\n",
    "\n",
    "    def _standardize_fit(self, X):\n",
    "        self.mu = X.mean(axis=0)\n",
    "        self.sigma = X.std(axis=0)\n",
    "        self.sigma[self.sigma < 1e-8] = 1.0\n",
    "        return (X - self.mu) / self.sigma\n",
    "\n",
    "    def _standardize_apply(self, X):\n",
    "        return (X - self.mu) / self.sigma\n",
    "\n",
    "    def fit(self, X_events, intervals_event_ranges, dY, verbose=True):\n",
    "        X = np.asarray(X_events, dtype=np.float64)\n",
    "        if X.ndim == 1:\n",
    "            X = X[:, None]\n",
    "        dY = np.asarray(dY, dtype=np.float64)\n",
    "        K = len(intervals_event_ranges)\n",
    "        D = X.shape[1]\n",
    "\n",
    "        if X.shape[0] == 0:\n",
    "            if verbose:\n",
    "                print(\"[trainer] No events detected; nothing to fit.\")\n",
    "            self.w = np.zeros(D, dtype=np.float64)\n",
    "            self.b = 0.0\n",
    "            self.alpha = 1.0\n",
    "            return self\n",
    "\n",
    "        x_std = self._standardize_fit(X)\n",
    "        self.w = np.zeros(D, dtype=np.float64)\n",
    "        self.b = 0.0\n",
    "\n",
    "        best = np.inf\n",
    "        no_improve = 0\n",
    "\n",
    "        for ep in range(1, self.epochs + 1):\n",
    "            grad_w = self.l2 * self.w\n",
    "            grad_b = self.l2 * self.b\n",
    "            loss = 0.0\n",
    "\n",
    "            for k, (a, b) in enumerate(intervals_event_ranges):\n",
    "                if a > b:\n",
    "                    loss += 0.5 * (0.0 - dY[k])**2\n",
    "                    continue\n",
    "                Xk = x_std[a:b+1, :]                    # (m,D)\n",
    "                zk = Xk @ self.w + self.b               # (m,)\n",
    "                yk = safe_softplus(zk)                  # (m,)\n",
    "                yhat = float(np.sum(yk))\n",
    "                err = (yhat - dY[k])\n",
    "                loss += 0.5 * (err ** 2)\n",
    "\n",
    "                sig = safe_sigmoid(zk)                  # (m,)\n",
    "                grad_w += err * (Xk * sig[:, None]).sum(axis=0)\n",
    "                grad_b += err * float(sig.sum())\n",
    "\n",
    "            # clip gradients\n",
    "            if self.clip is not None:\n",
    "                gnorm = float(np.sqrt((grad_w**2).sum() + grad_b*grad_b))\n",
    "                if gnorm > self.clip and gnorm > 0:\n",
    "                    scale = self.clip / gnorm\n",
    "                    grad_w *= scale\n",
    "                    grad_b *= scale\n",
    "            else:\n",
    "                gnorm = float(np.sqrt((grad_w**2).sum() + grad_b*grad_b))\n",
    "\n",
    "            # step\n",
    "            self.w -= self.lr * grad_w\n",
    "            self.b -= self.lr * grad_b\n",
    "\n",
    "            tot = loss + 0.5*self.l2*(float((self.w**2).sum()) + self.b*self.b)\n",
    "            if verbose and (ep == 1 or ep % self.report_every == 0 or ep == self.epochs):\n",
    "                print(f\"[ep {ep:04d}] loss={tot:.6f}  |w|={np.linalg.norm(self.w):.4f}  b={self.b:.4f}  |grad|≈{gnorm:.4f}\")\n",
    "\n",
    "            if tot + 1e-10 < best:\n",
    "                best = tot\n",
    "                no_improve = 0\n",
    "            else:\n",
    "                no_improve += 1\n",
    "                if no_improve >= self.early_stop_patience:\n",
    "                    if verbose:\n",
    "                        print(f\"[early-stop] best_loss={best:.6f}\")\n",
    "                    break\n",
    "\n",
    "        # ---- post-hoc calibration (non-negative scalar) ----\n",
    "        # α = argmin_α Σ_k (α * inc_pred_raw_k - dY_k)^2  => closed form\n",
    "        inc_pred_raw = self._interval_preds_raw(x_std, intervals_event_ranges)  # (K,)\n",
    "        denom = float((inc_pred_raw**2).sum())\n",
    "        numer = float((inc_pred_raw * dY).sum())\n",
    "        self.alpha = 1.0 if denom == 0 else max(0.0, numer / denom)\n",
    "        if verbose:\n",
    "            print(f\"[calibration] alpha = {self.alpha:.6f}\")\n",
    "        return self\n",
    "\n",
    "    def _interval_preds_raw(self, x_std, intervals_event_ranges):\n",
    "        # no alpha scaling here\n",
    "        yevt = safe_softplus(x_std @ self.w + self.b)\n",
    "        out = np.zeros(len(intervals_event_ranges), dtype=np.float64)\n",
    "        for k, (a, b) in enumerate(intervals_event_ranges):\n",
    "            out[k] = 0.0 if a > b else float(yevt[a:b+1].sum())\n",
    "        return out\n",
    "\n",
    "    def predict_events(self, X_events):\n",
    "        X = np.asarray(X_events, dtype=np.float64)\n",
    "        if X.ndim == 1:\n",
    "            X = X[:, None]\n",
    "        if X.shape[0] == 0:\n",
    "            return np.array([], dtype=np.float64)\n",
    "        x_std = self._standardize_apply(X)\n",
    "        yevt = safe_softplus(x_std @ self.w + self.b)\n",
    "        return self.alpha * yevt  # scaled\n",
    "\n",
    "# ---------- map label checkpoints -> contiguous event ranges ----------\n",
    "def _build_intervals_from_checkpoints(event_frames, label_frames):\n",
    "    event_frames = np.asarray(event_frames, dtype=int)\n",
    "    intervals = []\n",
    "    a = 0\n",
    "    N = len(event_frames)\n",
    "    for f_lab in label_frames:\n",
    "        b = a - 1\n",
    "        while b + 1 < N and event_frames[b + 1] <= f_lab:\n",
    "            b += 1\n",
    "        intervals.append((a, max(a - 1, b)))\n",
    "        a = b + 1\n",
    "    return intervals\n",
    "\n",
    "# =============================== MAIN ===============================\n",
    "def run_and_train(\n",
    "    video_path=\"../.local_data/perdue_rgb_video2_061725.mp4\",\n",
    "    model_path=\"models/best.pt\",\n",
    "    csv_path=\"../.local_data/counts_xy_true_vs_yolo_series.csv\",\n",
    "    out_path=\"object_counting_output.avi\",\n",
    "    line_points=[(320, 560), (1820, 540)],\n",
    "    draw=True\n",
    "    ):\n",
    "    # -------- labels --------\n",
    "    labels = pd.read_csv(csv_path)\n",
    "    if \"frame_index\" not in labels or \"true_count\" not in labels:\n",
    "        raise ValueError(\"CSV must have columns: frame_index, true_count (optionally true_count).\")\n",
    "    labels = labels.sort_values(\"frame_index\").reset_index(drop=True)\n",
    "\n",
    "    L_frames = labels[\"frame_index\"].to_numpy().astype(int)\n",
    "    cum_true = labels[\"true_count\"].to_numpy().astype(float)\n",
    "    dY = np.diff(np.concatenate([[0.0], cum_true]))  # increments\n",
    "\n",
    "    # -------- pass 1: collect events (no writing) --------\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    assert cap.isOpened(), \"Error reading video file\"\n",
    "    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
    "\n",
    "    A = tuple(map(int, line_points[0]))\n",
    "    B = tuple(map(int, line_points[1]))\n",
    "\n",
    "    yolo = YOLO(model_path)\n",
    "    last_center = {}\n",
    "\n",
    "    event_frames = []       # (N,)\n",
    "    event_feats = []        # (N, D)\n",
    "    f_idx = -1\n",
    "    if draw:\n",
    "        print_every = 1000\n",
    "    else:\n",
    "        print_every = 2000\n",
    "\n",
    "    while True:\n",
    "        ok, frame = cap.read()\n",
    "        if not ok:\n",
    "            break\n",
    "        f_idx += 1\n",
    "        if f_idx % print_every == 0:\n",
    "            print(f\"{f_idx}\")\n",
    "\n",
    "        results = yolo.track(frame, persist=True, verbose=False)\n",
    "        r = results[0]\n",
    "        boxes = getattr(r, \"boxes\", None)\n",
    "\n",
    "        if boxes is not None and boxes.id is not None:\n",
    "            ids = boxes.id.cpu().numpy().astype(int)\n",
    "            xyxy = boxes.xyxy.cpu().numpy()  # (N,4)\n",
    "\n",
    "            for tid, bb in zip(ids, xyxy):\n",
    "                x1, y1, x2, y2 = bb.astype(int)\n",
    "                cx = (x1 + x2) // 2\n",
    "                cy = (y1 + y2) // 2\n",
    "                curr = (int(cx), int(cy))\n",
    "                prev = last_center.get(tid, None)\n",
    "\n",
    "                if prev is not None:\n",
    "                    s_prev = _line_side(prev, A, B)\n",
    "                    s_curr = _line_side(curr, A, B)\n",
    "                    crossed = (s_prev != 0 and s_curr != 0 and s_prev != s_curr) or \\\n",
    "                              _segments_intersect(prev, curr, A, B)\n",
    "                    if crossed:\n",
    "                        xx1, yy1 = max(0, x1), max(0, y1)\n",
    "                        xx2, yy2 = min(w, x2), min(h, y2)\n",
    "                        roi = frame[yy1:yy2, xx1:xx2]\n",
    "                        feat = get_features(roi)      # shape (D,)\n",
    "                        event_frames.append(f_idx)\n",
    "                        event_feats.append(feat.astype(np.float64))\n",
    "\n",
    "                last_center[tid] = curr\n",
    "\n",
    "    cap.release()\n",
    "    total_frames = f_idx + 1\n",
    "    event_frames = np.asarray(event_frames, dtype=int)\n",
    "    event_feats  = np.asarray(event_feats,  dtype=np.float64)  # shape (N,D) or (0,)\n",
    "\n",
    "    # ---- intervals over events for each labeled checkpoint ----\n",
    "    intervals = _build_intervals_from_checkpoints(event_frames, L_frames)\n",
    "\n",
    "    # -------- train (verbose) --------\n",
    "    trainer = GroupedSoftplusSGDVector(\n",
    "        lr=3e-3, epochs=800, l2=1e-5, clip=2.0, report_every=25, early_stop_patience=50\n",
    "    )\n",
    "    trainer.fit(event_feats, intervals, dY, verbose=True)\n",
    "\n",
    "    # raw predictions per interval and cumulative (with alpha)\n",
    "    yhat_evt = trainer.predict_events(event_feats)             # (N,)\n",
    "    inc_pred = []\n",
    "    for a, b in intervals:\n",
    "        inc_pred.append(0.0 if a > b else float(np.sum(yhat_evt[a:b+1])))\n",
    "    inc_pred = np.asarray(inc_pred, dtype=np.float64)\n",
    "    cum_pred = np.cumsum(inc_pred)\n",
    "\n",
    "    # -------- metrics --------\n",
    "    def rmse(x, y): return float(np.sqrt(np.mean((np.asarray(x) - np.asarray(y))**2))) if len(x) else np.nan\n",
    "    def mae(x, y):  return float(np.mean(np.abs(np.asarray(x) - np.asarray(y))))       if len(x) else np.nan\n",
    "\n",
    "    metrics = {\n",
    "        \"events_total\": int(event_feats.shape[0]),\n",
    "        \"feature_dim\": int(event_feats.shape[1]) if event_feats.ndim == 2 and event_feats.size else 0,\n",
    "        \"intervals\": int(len(intervals)),\n",
    "        \"interval_RMSE\": rmse(inc_pred, dY),\n",
    "        \"interval_MAE\":  mae(inc_pred, dY),\n",
    "        \"cumulative_RMSE\": rmse(cum_pred, cum_true),\n",
    "        \"cumulative_MAE\":  mae(cum_pred, cum_true),\n",
    "        \"calibration_alpha\": float(trainer.alpha),\n",
    "        \"empty_intervals_with_positive_truth\":\n",
    "            int(sum(1 for (a,b),dy in zip(intervals, dY) if a > b and dy > 0))\n",
    "    }\n",
    "\n",
    "    print(\"\\n=== weak-supervision evaluation ===\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k:>36}: {v}\")\n",
    "\n",
    "    report = pd.DataFrame({\n",
    "        \"frame_index\": L_frames,\n",
    "        \"inc_true\": dY,\n",
    "        \"inc_pred\": inc_pred,\n",
    "        \"cum_true\": cum_true,\n",
    "        \"cum_pred\": cum_pred\n",
    "    })\n",
    "    print(\"\\ncheckpoint report (head):\\n\", report.head())\n",
    "\n",
    "    # -------- build per-frame overlays and write output video (pass 2) --------\n",
    "    # cum_pred_by_frame: accumulate event outputs up to each frame\n",
    "    cum_pred_by_frame = np.zeros(total_frames, dtype=np.float64)\n",
    "    if yhat_evt.size > 0:\n",
    "        # two-pointer accumulate\n",
    "        ef = event_frames\n",
    "        ev = yhat_evt\n",
    "        j = 0\n",
    "        running = 0.0\n",
    "        for f in range(total_frames):\n",
    "            while j < len(ef) and ef[j] <= f:\n",
    "                running += ev[j]\n",
    "                j += 1\n",
    "            cum_pred_by_frame[f] = running\n",
    "\n",
    "    # cum_true_by_frame: last known label up to frame f\n",
    "    cum_true_by_frame = np.zeros(total_frames, dtype=np.float64)\n",
    "    k = 0\n",
    "    last = 0.0\n",
    "    for f in range(total_frames):\n",
    "        while k < len(L_frames) and L_frames[k] == f:\n",
    "            last = cum_true[k]\n",
    "            k += 1\n",
    "        cum_true_by_frame[f] = last\n",
    "\n",
    "    # write overlay video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    assert cap.isOpened(), \"Error re-opening video file\"\n",
    "    writer = cv2.VideoWriter(out_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "    f_idx = -1\n",
    "    while True:\n",
    "        ok, frame = cap.read()\n",
    "        if not ok:\n",
    "            break\n",
    "        f_idx += 1\n",
    "\n",
    "        # draw counter line\n",
    "        cv2.line(frame, tuple(line_points[0]), tuple(line_points[1]), (0, 0, 255), 2)\n",
    "\n",
    "        # overlay counts\n",
    "        true_here = cum_true_by_frame[f_idx]\n",
    "        pred_here = cum_pred_by_frame[f_idx]\n",
    "        txt1 = f\"True cum: {true_here:.0f}\"\n",
    "        txt2 = f\"Pred cum: {pred_here:.1f}\"\n",
    "\n",
    "        cv2.putText(frame, txt1, (30, 50), font, 1.0, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(frame, txt2, (30, 90), font, 1.0, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        writer.write(frame)\n",
    "\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    return trainer, (event_frames, event_feats), report, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cef099",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer, event_tuple, report, metrics = run_and_train(\n",
    "    #video_path=\"../.local_data/perdue_rgb_video2_061725.mp4\",\n",
    "    #model_path=\"models/best.pt\",\n",
    "    #csv_path=\"../.local_data/counts_xy_true_vs_yolo_series.csv\",\n",
    "    out_path=\"object_counting_output.avi\",\n",
    "    line_points=[(320, 560), (1820, 540)],\n",
    "    draw=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d341bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "true_pred = report.values[:,-2:]\n",
    "\n",
    "plt.plot(true_pred)\n",
    "plt.show()\n",
    "\n",
    "batch_acc = []\n",
    "last_true = 0\n",
    "last_pred = 0\n",
    "b_id = 0\n",
    "\n",
    "for i in range(true_pred.shape[0]):\n",
    "    if((true_pred[i,0] >= (b_id+1)*100) or i==true_pred.shape[0]-1):\n",
    "\n",
    "        batch_acc.append(\n",
    "            (\n",
    "                (true_pred[i,1]-last_pred)\n",
    "                -\n",
    "                (true_pred[i,0]-last_true)\n",
    "            )\n",
    "        )\n",
    "        last_pred = true_pred[i,1]\n",
    "        last_true = true_pred[i,0]\n",
    "        b_id+=1\n",
    "\n",
    "batch_acc = np.asarray(batch_acc, dtype=np.float32)\n",
    "\n",
    "plt.plot(batch_acc)\n",
    "plt.show()\n",
    "\n",
    "print(np.mean(batch_acc))\n",
    "print(np.std(batch_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ade23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_forward_eval(event_frames, event_feats, L_frames, cum_true, TrainerCtor, verbose=False):\n",
    "    \"\"\"\n",
    "    event_frames: (N,) frame index of each crossing event (sorted)\n",
    "    event_feats:  (N,D) features per event (from your get_features)\n",
    "    L_frames:     (K,) labeled frame indices (sorted)\n",
    "    cum_true:     (K,) cumulative true counts at those frames\n",
    "    TrainerCtor:  callable that returns a *fresh* unfit trainer (e.g., GroupedSoftplusSGDVector)\n",
    "    \"\"\"\n",
    "    # labels as increments\n",
    "    dY = np.diff(np.concatenate([[0.0], cum_true]))\n",
    "    # map checkpoints -> contiguous event-index ranges\n",
    "    intervals = _build_intervals_from_checkpoints(event_frames, L_frames)\n",
    "\n",
    "    K = len(intervals)\n",
    "    inc_pred = np.zeros(K, dtype=float)\n",
    "\n",
    "    for t in range(K):\n",
    "        # training subset = intervals [0 .. t-1]\n",
    "        train_intervals = intervals[:t]\n",
    "        # find last event index included in training (prefix, so contiguous)\n",
    "        last_train_event = max((b for a,b in train_intervals if a <= b), default=-1)\n",
    "        if last_train_event < 0:\n",
    "            # no events yet -> can't learn; predict 0 for first interval\n",
    "            inc_pred[t] = 0.0\n",
    "            continue\n",
    "\n",
    "        X_train = event_feats[:last_train_event+1]\n",
    "        # intervals are already 0..last_train_event, so indices remain valid\n",
    "        dY_train = dY[:t]\n",
    "\n",
    "        trainer = TrainerCtor()\n",
    "        trainer.fit(X_train, train_intervals, dY_train, verbose=False)\n",
    "\n",
    "        # predict interval t using only its events\n",
    "        a, b = intervals[t]\n",
    "        if a > b:\n",
    "            inc_pred[t] = 0.0\n",
    "        else:\n",
    "            y_evt = trainer.predict_events(event_feats[a:b+1])\n",
    "            inc_pred[t] = float(y_evt.sum())\n",
    "\n",
    "        if verbose and (t % max(1, K//10) == 0):\n",
    "            print(f\"[WF] done {t+1}/{K}\")\n",
    "\n",
    "    cum_pred = inc_pred.cumsum()\n",
    "\n",
    "    def rmse(x, y): return float((( (x - y)**2 ).mean())**0.5)\n",
    "    def mae(x, y):  return float(np.abs(x - y).mean())\n",
    "\n",
    "    metrics = {\n",
    "        \"WF_interval_RMSE\": rmse(inc_pred, dY),\n",
    "        \"WF_interval_MAE\":  mae(inc_pred, dY),\n",
    "        \"WF_cumulative_RMSE\": rmse(cum_pred, cum_true),\n",
    "        \"WF_cumulative_MAE\":  mae(cum_pred, cum_true),\n",
    "    }\n",
    "    return inc_pred, cum_pred, metrics\n",
    "\n",
    "\n",
    "inc_pred, cum_pred, metrics = walk_forward_eval(event_tuple[0], event_tuple[1], report['frame_index'].values, report['cum_true'].values, \n",
    "                                                TrainerCtor=lambda: GroupedSoftplusSGDVector(lr=3e-3, epochs=800, l2=1e-5, clip=2.0,\n",
    "                                                 report_every=50, early_stop_patience=50),\n",
    "                                                 verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcb9837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wf_trues = report['inc_true'].values\n",
    "wf_preds = inc_pred\n",
    "\n",
    "errors = np.empty(wf_trues.shape[0], dtype=np.float32)\n",
    "\n",
    "\n",
    "for i in range(true_pred.shape[0]):\n",
    "    \n",
    "    errors[i] = (\n",
    "        (wf_preds[i])\n",
    "        -\n",
    "        (wf_trues[i])\n",
    "    )#))\n",
    "\n",
    "\n",
    "#plt.plot(errors)\n",
    "#plt.title('errors')\n",
    "#plt.show()\n",
    "\n",
    "n= 4\n",
    "\n",
    "err_test    = np.full(errors.shape[0], np.nan, dtype=np.float32)\n",
    "err_test_q  = np.full(errors.shape[0], np.nan, dtype=np.float32)\n",
    "err_test_qma= np.full(errors.shape[0], np.nan, dtype=np.float32)\n",
    "err_test_q99 = np.full((errors.shape[0],n), np.nan, dtype=np.float32)\n",
    "err_test_q50 = np.full((errors.shape[0],n), np.nan, dtype=np.float32)\n",
    "err_test_q10 = np.full((errors.shape[0],n), np.nan, dtype=np.float32)\n",
    "\n",
    "\n",
    "for i in range(err_test.shape[0]):\n",
    "    err_test[i] = abs(errors[i])/(wf_trues[i])\n",
    "\n",
    "qk = 30\n",
    "\n",
    "for i in range(qk, err_test.shape[0]):\n",
    "    err_test_q[i] = np.mean(err_test[i-qk:i])#, q=0.5)\n",
    "\n",
    "k = 30\n",
    "\n",
    "for i in range(qk+k, err_test.shape[0]):\n",
    "    err_test_qma[i] = np.quantile(err_test_q[i-k:i], q=0.99)\n",
    "\n",
    "kn = 6\n",
    "\n",
    "for j in range(n):\n",
    "    k_off = kn*(j+1)\n",
    "    for i in range(k_off, err_test.shape[0]-2):\n",
    "        err_test_q99[i, j] = (np.quantile(\n",
    "            (100/np.sum(wf_trues[i-k_off:i]))*(np.abs(errors[i-k_off:i])), \n",
    "            q=0.99\n",
    "        ))\n",
    "    for i in range(k_off, err_test.shape[0]-2):\n",
    "        err_test_q50[i, j] = (np.quantile(\n",
    "            (100/np.sum(wf_trues[i-k_off:i]))*(np.abs(errors[i-k_off:i])), \n",
    "            q=0.50\n",
    "        ))\n",
    "    for i in range(k_off, err_test.shape[0]-2):\n",
    "        err_test_q10[i, j] = (np.quantile(\n",
    "            (100/np.sum(wf_trues[i-k_off:i]))*(np.abs(errors[i-k_off:i])), \n",
    "            q=0.10\n",
    "        ))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(err_test)\n",
    "plt.title('Miscounts through WF Evaluation\\nWeighted by increment count')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(err_test_q)\n",
    "plt.title(\n",
    "    'Smoothed (30) weighted Miscounts through WF Evaluation')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(err_test_qma)\n",
    "plt.title(\n",
    "    'Worst (Q99, 30) across Smoothed weighted miscounts\\nThrough WF Evaluation')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "err_test_q99[:10,:] = np.nan\n",
    "err_test_q50[:10,:] = np.nan\n",
    "err_test_q10[:10,:] = np.nan\n",
    "\n",
    "#--------------------------\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def maroon_ramp(n_cols,\n",
    "                light_rgb=(245, 225, 225),   # very light maroon-ish\n",
    "                dark_rgb=(215, 135, 135)):   # darker maroon-ish\n",
    "    # Return a list of n_cols colors between light and dark (0–1 floats for mpl)\n",
    "    a = np.array(light_rgb, dtype=float) / 255.0\n",
    "    b = np.array(dark_rgb,  dtype=float) / 255.0\n",
    "    if n_cols == 1:\n",
    "        return [a]\n",
    "    alphas = np.linspace(0, 1, n_cols)\n",
    "    return [tuple(a*(1-t) + b*t) for t in alphas]\n",
    "\n",
    "# Assume qs_clip is shape (T, D) where each column is a series to plot\n",
    "D = err_test_q99.shape[1]\n",
    "colors = maroon_ramp(D, light_rgb=(245,225,225), dark_rgb=(165,35,35))\n",
    "#---------------------------\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_prop_cycle(color=colors)  # set the colors for successive plot lines\n",
    "\n",
    "\n",
    "\n",
    "ax.plot(err_test_q99)                 # this now uses your maroon gradient\n",
    "ax.set_title(\n",
    "    'Worst possible # miscounts expected per batch (of 100)\\nThroughout WF Evaluation')  # your original title\n",
    "#ax.grid(True, alpha=0.2)\n",
    "\n",
    "plt.show()\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_prop_cycle(color=colors)  # set the colors for successive plot lines\n",
    "\n",
    "ax.plot(err_test_q50)                 # this now uses your maroon gradient\n",
    "ax.set_title(\n",
    "    'Most likely # miscounts expected per batch (of 100)\\nThroughout WF Evaluation')  # your original title\n",
    "#ax.grid(True, alpha=0.2)\n",
    "\n",
    "plt.show()\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_prop_cycle(color=colors)  # set the colors for successive plot lines\n",
    "\n",
    "ax.plot(err_test_q10)                 # this now uses your maroon gradient\n",
    "ax.set_title(\n",
    "    'Best 10% # miscounts expected per batch (of 100)\\nThroughout WF Evaluation'\n",
    ")  # your original title\n",
    "#ax.grid(True, alpha=0.2)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9318b8",
   "metadata": {},
   "source": [
    "$i \\in \\text{WF}, \\quad w_i = \\frac{|y_i-\\hat{y}_i|}{y_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c102cd",
   "metadata": {},
   "source": [
    "$k_1=30, i \\in \\{k_1,\\dots, \\text{WF}\\}, s_i = \\frac{1}{k_1}\\sum^{i}_{i-k_1} w_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff85f10",
   "metadata": {},
   "source": [
    "$k_1=k_2=30, i \\in \\{k_1+k_2,\\dots, \\text{WF}\\},$\n",
    "\n",
    "$ Q_{0.99}(\\{s_i : j\\in \\{i-k_2,\\dots, \\text{WF} \\} \\})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d662a5ac",
   "metadata": {},
   "source": [
    "$b \\in \\{1,2,3,4\\}, i \\in \\{6b, \\dots, \\text{WF}\\}$\n",
    "\n",
    "$\\frac{100}{\\sum^{i}_{n=i-6b} y_n} Q_{0.99}(\\{(|y_j - \\hat{y}_j|) : j\\in \\{i-6b,\\dots, i \\} \\})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517aa6e8",
   "metadata": {},
   "source": [
    "$b \\in \\{1,2,3,4\\}, i \\in \\{6b, \\dots, \\text{WF}\\}$\n",
    "\n",
    "$\\frac{100}{\\sum^{i}_{n=i-6b} y_n} \\text{median}(\\{|y_j - \\hat{y}_j| : j\\in \\{i-6b,\\dots, i \\} \\})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fe639e",
   "metadata": {},
   "source": [
    "$b \\in \\{1,2,3,4\\}, i \\in \\{6b, \\dots, \\text{WF}\\}$\n",
    "\n",
    "$\\frac{100}{\\sum^{i}_{n=i-6b} y_n} Q_{0.10}(\\{(|y_j - \\hat{y}_j|) : j\\in \\{i-6b,\\dots, i \\} \\})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9423b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#err_test\n",
    "#err_test_q\n",
    "#err_test_qma\n",
    "#err_test_q99\n",
    "#err_test_q50\n",
    "#err_test_q10\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('wf_perf/pkl_init/err_test.pkl', 'wb') as file:\n",
    "    pickle.dump(err_test, file)\n",
    "with open('wf_perf/pkl_init/err_test_q.pkl', 'wb') as file:\n",
    "    pickle.dump(err_test_q, file)\n",
    "with open('wf_perf/pkl_init/err_test_qma.pkl', 'wb') as file:\n",
    "    pickle.dump(err_test_qma, file)\n",
    "with open('wf_perf/pkl_init/err_test_q99.pkl', 'wb') as file:\n",
    "    pickle.dump(err_test_q99, file)\n",
    "with open('wf_perf/pkl_init/err_test_q50.pkl', 'wb') as file:\n",
    "    pickle.dump(err_test_q50, file)\n",
    "with open('wf_perf/pkl_init/err_test_q10.pkl', 'wb') as file:\n",
    "    pickle.dump(err_test_q10, file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
