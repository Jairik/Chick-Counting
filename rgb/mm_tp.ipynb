{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2167b0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "from ultralytics import solutions  # kept for compatibility\n",
    "\n",
    "# -----------------------------\n",
    "# numerically-safe activations\n",
    "# -----------------------------\n",
    "def safe_softplus(z):\n",
    "    z = np.asarray(z, dtype=np.float64)\n",
    "    return np.log1p(np.exp(-np.abs(z))) + np.maximum(z, 0.0)\n",
    "\n",
    "def safe_sigmoid(z):\n",
    "    z = np.asarray(z, dtype=np.float64)\n",
    "    out = np.empty_like(z, dtype=np.float64)\n",
    "    pos = z >= 0\n",
    "    out[pos]  = 1.0 / (1.0 + np.exp(-z[pos]))\n",
    "    e = np.exp(z[~pos])\n",
    "    out[~pos] = e / (1.0 + e)\n",
    "    return out\n",
    "\n",
    "# -----------------------------\n",
    "# geometry helpers (center-line crossing)\n",
    "# -----------------------------\n",
    "def _orient(a, b, c):\n",
    "    return (b[0]-a[0])*(c[1]-a[1]) - (b[1]-a[1])*(c[0]-a[0])\n",
    "\n",
    "def _line_side(p, a, b):\n",
    "    s = _orient(a, b, p)\n",
    "    return 0 if s == 0 else (1 if s > 0 else -1)\n",
    "\n",
    "def _segments_intersect(p1, p2, q1, q2):\n",
    "    o1 = _orient(p1, p2, q1)\n",
    "    o2 = _orient(p1, p2, q2)\n",
    "    o3 = _orient(q1, q2, p1)\n",
    "    o4 = _orient(q1, q2, p2)\n",
    "    if (o1 == 0 and o2 == 0 and o3 == 0 and o4 == 0):\n",
    "        return (min(p1[0], p2[0]) <= max(q1[0], q2[0]) and\n",
    "                min(q1[0], q2[0]) <= max(p1[0], p2[0]) and\n",
    "                min(p1[1], p2[1]) <= max(q1[1], q2[1]) and\n",
    "                min(q1[1], q2[1]) <= max(p1[1], p2[1]))\n",
    "    return (o1 == 0 or o2 == 0 or np.sign(o1) != np.sign(o2)) and \\\n",
    "           (o3 == 0 or o4 == 0 or np.sign(o3) != np.sign(o4))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# feature extractor (vector)\n",
    "#   - bbox area (normalized)\n",
    "#   - aspect ratio (w/h)\n",
    "#   - squareness = min(w,h)/max(w,h)\n",
    "#   - yellow metrics in HSV:\n",
    "#       * fraction of yellow pixels\n",
    "#       * mean S within yellow\n",
    "#       * mean V within yellow\n",
    "# -----------------------------\n",
    "import utils.get_bounding_box_features as bbf\n",
    "import utils.yellow_masks as ym\n",
    "def get_features(roi_bgr: np.ndarray) -> np.ndarray:\n",
    "    if roi_bgr.size == 0:\n",
    "        return np.zeros(6, dtype=np.float64)\n",
    "    h, w = roi_bgr.shape[:2]\n",
    "    area = float(h * w)\n",
    "    if area <= 1:\n",
    "        return np.zeros(6, dtype=np.float64)\n",
    "\n",
    "    # geometry\n",
    "    area_norm = area / 1e4                      # rough normalization scale\n",
    "    aspect = (w / max(h, 1.0))\n",
    "    square = min(w, h) / max(w, h)\n",
    "\n",
    "    # color (HSV)\n",
    "    hsv = cv2.cvtColor(roi_bgr, cv2.COLOR_BGR2HSV)\n",
    "    H, S, V = hsv[..., 0], hsv[..., 1], hsv[..., 2]\n",
    "\n",
    "    # yellow mask (OpenCV H in [0,179]): approx 20-40°, adjust if needed\n",
    "    yellow_lo, yellow_hi = 20, 40\n",
    "    S_lo, V_lo = 70, 100  # avoid very desaturated/dim pixels\n",
    "    ymask = (H >= yellow_lo) & (H <= yellow_hi) & (S >= S_lo) & (V >= V_lo)\n",
    "\n",
    "    y_count = float(np.count_nonzero(ymask))\n",
    "    y_frac  = y_count / area\n",
    "    if y_count > 0:\n",
    "        y_mean_s = float(S[ymask].mean()) / 255.0\n",
    "        y_mean_v = float(V[ymask].mean()) / 255.0\n",
    "    else:\n",
    "        y_mean_s = 0.0\n",
    "        y_mean_v = 0.0\n",
    "\n",
    "    # overall brightness can still help a bit\n",
    "    v_mean = float(V.mean()) / 255.0\n",
    "    \n",
    "    box = ym.get_yellow_mask(img=roi_bgr, normalize=True)\n",
    "    \n",
    "    area = bbf.get_area(box)  # Total area of the bounding box\n",
    "    mean_yellow = bbf.get_mean_yellow(box)  # Mean temperature in the bounding box\n",
    "    pthresh_o = bbf.get_pixels_over_threshold(box, 0.8)  # Standard deviation of temperatures\n",
    "    pthresh_u = bbf.get_pixels_under_threshold(box, 0.8)  # Variance of temperatures\n",
    "    y_range = bbf.get_yellow_range(box)  # Range of temperatures\n",
    "    y_std = bbf.get_yellow_std(box)  # Relative count of pixels over 30C\n",
    "    y_var = bbf.get_yellow_variance(box)  # Relative count of pixels under 30C\n",
    "    mdft = bbf.get_mean_distance_from_threshold(box, threshold=.8)  # Mean distance from 30C\n",
    "    asp_rat = bbf.get_aspect_ratio(box)  # Aspect ratio of the bounding box\n",
    "\n",
    "    return np.array([ area_norm, aspect, square, y_frac, y_mean_s, y_mean_v, v_mean, #], dtype=np.float64)\n",
    "                     area, mean_yellow, pthresh_o, pthresh_u, y_range, y_std, y_var, mdft, asp_rat], dtype=np.float64)\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import utils.get_bounding_box_features as bbf\n",
    "import utils.yellow_masks as ym\n",
    "\n",
    "def _safe_div(a, b, default=0.0):\n",
    "    return float(a) / float(b) if b not in (0, 0.0) else float(default)\n",
    "\n",
    "def _entropy_from_values(vals, bins=32):\n",
    "    if vals.size == 0:\n",
    "        return 0.0\n",
    "    hist, _ = np.histogram(vals, bins=bins, range=(0, 255), density=False)\n",
    "    p = hist.astype(np.float64)\n",
    "    Z = p.sum()\n",
    "    if Z <= 0:\n",
    "        return 0.0\n",
    "    p /= Z\n",
    "    p = p[p > 0]\n",
    "    return float(-(p * np.log2(p)).sum())\n",
    "\n",
    "def get_features2(roi_bgr: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Feature vector (length 32) summarizing yellow coverage, topology, texture,\n",
    "    and spatial mass within a chick bounding box (BGR ROI).\n",
    "    Returns zeros if ROI is empty or degenerate.\n",
    "    \"\"\"\n",
    "    # -------------------- safety checks --------------------\n",
    "    if roi_bgr is None or roi_bgr.size == 0:\n",
    "        return np.zeros(32, dtype=np.float64)\n",
    "\n",
    "    h, w = roi_bgr.shape[:2]\n",
    "    area_bbox = int(h) * int(w)\n",
    "    if area_bbox <= 1:\n",
    "        return np.zeros(32, dtype=np.float64)\n",
    "\n",
    "    # -------------------- geometry --------------------\n",
    "    area_norm = area_bbox / 1e4  # rough scale for stability across ROI sizes\n",
    "    aspect = _safe_div(w, max(h, 1.0), 0.0)\n",
    "    squareness = _safe_div(min(w, h), max(w, h), 0.0)\n",
    "\n",
    "    # -------------------- color masks (HSV) --------------------\n",
    "    hsv = cv2.cvtColor(roi_bgr, cv2.COLOR_BGR2HSV)\n",
    "    H, S, V = hsv[..., 0], hsv[..., 1], hsv[..., 2]\n",
    "\n",
    "    # \"Yellow\" band in OpenCV HSV: H in [20, 40], with S and V floors to avoid dull/dark\n",
    "    yellow_lo, yellow_hi = 20, 40\n",
    "    S_lo, V_lo = 70, 100\n",
    "    ymask = (H >= yellow_lo) & (H <= yellow_hi) & (S >= S_lo) & (V >= V_lo)\n",
    "\n",
    "    y_count = int(np.count_nonzero(ymask))\n",
    "    y_frac = _safe_div(y_count, area_bbox, 0.0)\n",
    "\n",
    "    # overall brightness (helps disambiguate lighting)\n",
    "    v_mean_overall = float(V.mean()) / 255.0\n",
    "\n",
    "    # stats INSIDE yellow only\n",
    "    if y_count > 0:\n",
    "        S_y = S[ymask].astype(np.float64)\n",
    "        V_y = V[ymask].astype(np.float64)\n",
    "\n",
    "        y_mean_s = float(S_y.mean()) / 255.0\n",
    "        y_mean_v = float(V_y.mean()) / 255.0\n",
    "\n",
    "        y_std_s = float(S_y.std(ddof=0)) / 255.0\n",
    "        y_std_v = float(V_y.std(ddof=0)) / 255.0\n",
    "\n",
    "        # Percentiles of V (contrast/illumination robustness)\n",
    "        y_p10_v = float(np.percentile(V_y, 10)) / 255.0\n",
    "        y_p50_v = float(np.percentile(V_y, 50)) / 255.0\n",
    "        y_p90_v = float(np.percentile(V_y, 90)) / 255.0\n",
    "\n",
    "        # Entropies (texture richness of S and V within yellow)\n",
    "        y_entropy_s = _entropy_from_values(S_y, bins=32)\n",
    "        y_entropy_v = _entropy_from_values(V_y, bins=32)\n",
    "    else:\n",
    "        y_mean_s = y_mean_v = 0.0\n",
    "        y_std_s = y_std_v = 0.0\n",
    "        y_p10_v = y_p50_v = y_p90_v = 0.0\n",
    "        y_entropy_s = y_entropy_v = 0.0\n",
    "\n",
    "    # -------------------- topology / shape on yellow mask --------------------\n",
    "    # Convert mask to uint8 for contour ops\n",
    "    ymask_u8 = ymask.astype(np.uint8) * 255\n",
    "\n",
    "    comp_count = 0\n",
    "    largest_comp_frac = 0.0\n",
    "    perimeter_norm = 0.0\n",
    "    solidity = 0.0\n",
    "    eccentricity = 0.0\n",
    "    orientation = 0.0  # principal axis angle normalized by pi to [-1, 1]\n",
    "    cx_norm = 0.0\n",
    "    cy_norm = 0.0\n",
    "\n",
    "    if y_count > 0:\n",
    "        contours, _ = cv2.findContours(ymask_u8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        comp_count = len(contours)\n",
    "\n",
    "        # Areas & perimeters\n",
    "        cont_areas = [cv2.contourArea(c) for c in contours] if contours else [0.0]\n",
    "        total_perimeter = sum(cv2.arcLength(c, True) for c in contours) if contours else 0.0\n",
    "        largest_area = max(cont_areas) if cont_areas else 0.0\n",
    "\n",
    "        largest_comp_frac = _safe_div(largest_area, y_count, 0.0)\n",
    "        perimeter_norm = _safe_div(total_perimeter, np.sqrt(area_bbox), 0.0)  # scale-invariant\n",
    "\n",
    "        # Solidity = area / convex_hull_area (summed over comps)\n",
    "        hull_area_sum = 0.0\n",
    "        for c in contours:\n",
    "            hull = cv2.convexHull(c)\n",
    "            hull_area_sum += cv2.contourArea(hull)\n",
    "        solidity = _safe_div(y_count, hull_area_sum, 0.0)\n",
    "\n",
    "        # Centroid and PCA-based eccentricity/orientation\n",
    "        ys, xs = np.nonzero(ymask)\n",
    "        if xs.size > 1:\n",
    "            # centroid\n",
    "            cx = xs.mean()\n",
    "            cy = ys.mean()\n",
    "            cx_norm = _safe_div(cx, (w - 1 if w > 1 else 1))\n",
    "            cy_norm = _safe_div(cy, (h - 1 if h > 1 else 1))\n",
    "\n",
    "            # covariance of coordinates\n",
    "            X = np.column_stack((xs - cx, ys - cy)).astype(np.float64)\n",
    "            C = np.dot(X.T, X) / max(X.shape[0] - 1, 1)\n",
    "            # eigen-decomposition\n",
    "            evals, evecs = np.linalg.eigh(C)  # sorted ascending\n",
    "            l2, l1 = evals[0], evals[1]  # l1 >= l2\n",
    "            if l1 > 1e-12:\n",
    "                eccentricity = float(np.sqrt(1.0 - (l2 / l1)))  # [0,1)\n",
    "                # principal direction is eigenvector of l1\n",
    "                v = evecs[:, 1]\n",
    "                angle = float(np.arctan2(v[1], v[0]))  # radians in [-pi, pi]\n",
    "                orientation = angle / np.pi  # normalize to [-1, 1]\n",
    "            else:\n",
    "                eccentricity = 0.0\n",
    "                orientation = 0.0\n",
    "\n",
    "    # -------------------- edges / texture within yellow --------------------\n",
    "    edge_density = 0.0\n",
    "    sobel_mean = 0.0\n",
    "    lap_var = 0.0\n",
    "\n",
    "    if y_count > 0:\n",
    "        # Use V channel for stable edges under hue shifts\n",
    "        canny = cv2.Canny(V, 50, 150)\n",
    "        edge_inside = np.logical_and(canny > 0, ymask)\n",
    "        edge_density = _safe_div(np.count_nonzero(edge_inside), y_count, 0.0)\n",
    "\n",
    "        # Sobel gradient magnitude (mean)\n",
    "        Gx = cv2.Sobel(V, cv2.CV_64F, 1, 0, ksize=3)\n",
    "        Gy = cv2.Sobel(V, cv2.CV_64F, 0, 1, ksize=3)\n",
    "        mag = np.hypot(Gx, Gy)\n",
    "        sobel_mean = float(mag[ymask].mean()) if y_count > 0 else 0.0\n",
    "        sobel_mean /= 255.0\n",
    "\n",
    "        # Laplacian focus measure (variance)\n",
    "        lap = cv2.Laplacian(V, cv2.CV_64F, ksize=3)\n",
    "        lv = lap[ymask]\n",
    "        lap_var = float(lv.var()) if lv.size > 0 else 0.0\n",
    "        # Normalize roughly by (255^2) to keep on [0,1]-ish scale\n",
    "        lap_var /= (255.0 * 255.0)\n",
    "\n",
    "    # -------------------- spatial mass distribution (quadrants) --------------------\n",
    "    q1 = q2 = q3 = q4 = 0.0\n",
    "    if y_count > 0:\n",
    "        mid_y = h // 2\n",
    "        mid_x = w // 2\n",
    "        # TL, TR, BL, BR\n",
    "        q1 = _safe_div(np.count_nonzero(ymask[0:mid_y, 0:mid_x]), y_count)\n",
    "        q2 = _safe_div(np.count_nonzero(ymask[0:mid_y, mid_x:w]), y_count)\n",
    "        q3 = _safe_div(np.count_nonzero(ymask[mid_y:h, 0:mid_x]), y_count)\n",
    "        q4 = _safe_div(np.count_nonzero(ymask[mid_y:h, mid_x:w]), y_count)\n",
    "\n",
    "    # -------------------- keep a few curated, non-redundant bbf/ym features --------------------\n",
    "    # We use your pipeline for consistency, but drop redundant items:\n",
    "    # - Drop bbf.get_area (duplicates area/coverage ideas),\n",
    "    # - Drop pthresh_under, y_std, y_var, asp_rat (covered by ours),\n",
    "    # - Keep mean_yellow, mdft, and y_range as complementary scalars.\n",
    "    box = ym.get_yellow_mask(img=roi_bgr, normalize=True)\n",
    "    mean_yellow = float(bbf.get_mean_yellow(box))\n",
    "    mdft = float(bbf.get_mean_distance_from_threshold(box, threshold=0.8))\n",
    "    y_range = float(bbf.get_yellow_range(box))\n",
    "\n",
    "    # -------------------- assemble final vector (32 dims) --------------------\n",
    "    feats = np.array([\n",
    "        # 1–3: geometry\n",
    "        area_norm, aspect, squareness,\n",
    "        # 4–7: yellow coverage & brightness\n",
    "        y_frac, y_mean_s, y_mean_v, v_mean_overall,\n",
    "        # 8–14: dispersion & entropy inside yellow\n",
    "        y_std_s, y_std_v, y_p10_v, y_p50_v, y_p90_v, y_entropy_s, y_entropy_v,\n",
    "        # 15–20: topology/shape\n",
    "        float(comp_count), largest_comp_frac, perimeter_norm, solidity, eccentricity, orientation,\n",
    "        # 21–23: edge/texture\n",
    "        edge_density, sobel_mean, lap_var,\n",
    "        # 24–29: centroid + quadrants\n",
    "        cx_norm, cy_norm, q1, q2, q3, q4,\n",
    "        # 30–32: curated from your bbf/ym\n",
    "        mean_yellow, mdft, y_range\n",
    "    ], dtype=np.float64)\n",
    "\n",
    "    # Sanity: ensure length 32\n",
    "    if feats.shape[0] != 32:\n",
    "        out = np.zeros(32, dtype=np.float64)\n",
    "        n = min(32, feats.shape[0])\n",
    "        out[:n] = feats[:n]\n",
    "        return out\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# vector head, softplus output, grouped loss, calibration\n",
    "# -------------------------------------------------------\n",
    "class GroupedSoftplusSGDVector:\n",
    "    \"\"\"\n",
    "    Event output y_i = softplus( w·x_i_std + b ) >= 0\n",
    "    Loss = Σ_k 0.5 * (Σ_{i∈Ik} y_i - dY_k)^2 + L2\n",
    "    After training, apply a non-negative scalar calibration α to match mass.\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=3e-3, epochs=800, l2=1e-5, clip=2.0, report_every=25, early_stop_patience=50):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.l2 = l2\n",
    "        self.clip = clip\n",
    "        self.report_every = report_every\n",
    "        self.early_stop_patience = early_stop_patience\n",
    "\n",
    "        self.w = None  # (D,)\n",
    "        self.b = 0.0\n",
    "        self.mu = None  # (D,)\n",
    "        self.sigma = None  # (D,)\n",
    "        self.alpha = 1.0  # final calibration\n",
    "\n",
    "    def _standardize_fit(self, X):\n",
    "        self.mu = X.mean(axis=0)\n",
    "        self.sigma = X.std(axis=0)\n",
    "        self.sigma[self.sigma < 1e-8] = 1.0\n",
    "        return (X - self.mu) / self.sigma\n",
    "\n",
    "    def _standardize_apply(self, X):\n",
    "        return (X - self.mu) / self.sigma\n",
    "\n",
    "    def fit(self, X_events, intervals_event_ranges, dY, verbose=True):\n",
    "        X = np.asarray(X_events, dtype=np.float64)\n",
    "        if X.ndim == 1:\n",
    "            X = X[:, None]\n",
    "        dY = np.asarray(dY, dtype=np.float64)\n",
    "        K = len(intervals_event_ranges)\n",
    "        D = X.shape[1]\n",
    "\n",
    "        if X.shape[0] == 0:\n",
    "            if verbose:\n",
    "                print(\"[trainer] No events detected; nothing to fit.\")\n",
    "            self.w = np.zeros(D, dtype=np.float64)\n",
    "            self.b = 0.0\n",
    "            self.alpha = 1.0\n",
    "            return self\n",
    "\n",
    "        x_std = self._standardize_fit(X)\n",
    "        self.w = np.zeros(D, dtype=np.float64)\n",
    "        self.b = 0.0\n",
    "\n",
    "        best = np.inf\n",
    "        no_improve = 0\n",
    "\n",
    "        for ep in range(1, self.epochs + 1):\n",
    "            grad_w = self.l2 * self.w\n",
    "            grad_b = self.l2 * self.b\n",
    "            loss = 0.0\n",
    "\n",
    "            for k, (a, b) in enumerate(intervals_event_ranges):\n",
    "                if a > b:\n",
    "                    loss += 0.5 * (0.0 - dY[k])**2\n",
    "                    continue\n",
    "                Xk = x_std[a:b+1, :]                    # (m,D)\n",
    "                zk = Xk @ self.w + self.b               # (m,)\n",
    "                yk = safe_softplus(zk)                  # (m,)\n",
    "                yhat = float(np.sum(yk))\n",
    "                err = (yhat - dY[k])\n",
    "                loss += 0.5 * (err ** 2)\n",
    "\n",
    "                sig = safe_sigmoid(zk)                  # (m,)\n",
    "                grad_w += err * (Xk * sig[:, None]).sum(axis=0)\n",
    "                grad_b += err * float(sig.sum())\n",
    "\n",
    "            # clip gradients\n",
    "            if self.clip is not None:\n",
    "                gnorm = float(np.sqrt((grad_w**2).sum() + grad_b*grad_b))\n",
    "                if gnorm > self.clip and gnorm > 0:\n",
    "                    scale = self.clip / gnorm\n",
    "                    grad_w *= scale\n",
    "                    grad_b *= scale\n",
    "            else:\n",
    "                gnorm = float(np.sqrt((grad_w**2).sum() + grad_b*grad_b))\n",
    "\n",
    "            # step\n",
    "            self.w -= self.lr * grad_w\n",
    "            self.b -= self.lr * grad_b\n",
    "\n",
    "            tot = loss + 0.5*self.l2*(float((self.w**2).sum()) + self.b*self.b)\n",
    "            if verbose and (ep == 1 or ep % self.report_every == 0 or ep == self.epochs):\n",
    "                print(f\"[ep {ep:04d}] loss={tot:.6f}  |w|={np.linalg.norm(self.w):.4f}  b={self.b:.4f}  |grad|≈{gnorm:.4f}\")\n",
    "\n",
    "            if tot + 1e-10 < best:\n",
    "                best = tot\n",
    "                no_improve = 0\n",
    "            else:\n",
    "                no_improve += 1\n",
    "                if no_improve >= self.early_stop_patience:\n",
    "                    if verbose:\n",
    "                        print(f\"[early-stop] best_loss={best:.6f}\")\n",
    "                    break\n",
    "            \n",
    "            #print('Finish @ epoch: ',self.epochs)\n",
    "\n",
    "        # ---- post-hoc calibration (non-negative scalar) ----\n",
    "        # α = argmin_α Σ_k (α * inc_pred_raw_k - dY_k)^2  => closed form\n",
    "        inc_pred_raw = self._interval_preds_raw(x_std, intervals_event_ranges)  # (K,)\n",
    "        denom = float((inc_pred_raw**2).sum())\n",
    "        numer = float((inc_pred_raw * dY).sum())\n",
    "        self.alpha = 1.0 if denom == 0 else max(0.0, numer / denom)\n",
    "        if verbose:\n",
    "            print(f\"[calibration] alpha = {self.alpha:.6f}\")\n",
    "        return self\n",
    "\n",
    "    def _interval_preds_raw(self, x_std, intervals_event_ranges):\n",
    "        # no alpha scaling here\n",
    "        yevt = safe_softplus(x_std @ self.w + self.b)\n",
    "        out = np.zeros(len(intervals_event_ranges), dtype=np.float64)\n",
    "        for k, (a, b) in enumerate(intervals_event_ranges):\n",
    "            out[k] = 0.0 if a > b else float(yevt[a:b+1].sum())\n",
    "        return out\n",
    "\n",
    "    def predict_events(self, X_events):\n",
    "        X = np.asarray(X_events, dtype=np.float64)\n",
    "        if X.ndim == 1:\n",
    "            X = X[:, None]\n",
    "        if X.shape[0] == 0:\n",
    "            return np.array([], dtype=np.float64)\n",
    "        x_std = self._standardize_apply(X)\n",
    "        yevt = safe_softplus(x_std @ self.w + self.b)\n",
    "        return self.alpha * yevt  # scaled\n",
    "\n",
    "# ---------- map label checkpoints -> contiguous event ranges ----------\n",
    "def _build_intervals_from_checkpoints(event_frames, label_frames):\n",
    "    event_frames = np.asarray(event_frames, dtype=int)\n",
    "    intervals = []\n",
    "    a = 0\n",
    "    N = len(event_frames)\n",
    "    for f_lab in label_frames:\n",
    "        b = a - 1\n",
    "        while b + 1 < N and event_frames[b + 1] <= f_lab:\n",
    "            b += 1\n",
    "        intervals.append((a, max(a - 1, b)))\n",
    "        a = b + 1\n",
    "    return intervals\n",
    "\n",
    "# =============================== MAIN ===============================\n",
    "def run_and_train(\n",
    "    video_path=\"../.local_data/perdue_rgb_video2_061725.mp4\",\n",
    "    model_path=\"models/best.pt\",\n",
    "    csv_path=\"../.local_data/counts_xy_true_vs_yolo_series.csv\",\n",
    "    out_path=\"object_counting_output.avi\",\n",
    "    line_points=[(320, 560), (1820, 540)],\n",
    "    draw=True,\n",
    "    output_video=False\n",
    "    ):\n",
    "    # -------- labels --------\n",
    "    labels = pd.read_csv(csv_path)\n",
    "    if \"frame_index\" not in labels or \"true_count\" not in labels:\n",
    "        raise ValueError(\"CSV must have columns: frame_index, true_count (optionally true_count).\")\n",
    "    labels = labels.sort_values(\"frame_index\").reset_index(drop=True)\n",
    "\n",
    "    L_frames = labels[\"frame_index\"].to_numpy().astype(int)\n",
    "    cum_true = labels[\"true_count\"].to_numpy().astype(float)\n",
    "    dY = np.diff(np.concatenate([[0.0], cum_true]))  # increments\n",
    "\n",
    "    # -------- pass 1: collect events (no writing) --------\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    assert cap.isOpened(), \"Error reading video file\"\n",
    "    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
    "\n",
    "    A = tuple(map(int, line_points[0]))\n",
    "    B = tuple(map(int, line_points[1]))\n",
    "\n",
    "    yolo = YOLO(model_path)\n",
    "    last_center = {}\n",
    "\n",
    "    event_frames = []       # (N,)\n",
    "    event_feats = []        # (N, D)\n",
    "    f_idx = -1\n",
    "    if draw:\n",
    "        print_every = 1000\n",
    "    else:\n",
    "        print_every = 2000\n",
    "\n",
    "    while True:\n",
    "        ok, frame = cap.read()\n",
    "        if not ok:\n",
    "            break\n",
    "        f_idx += 1\n",
    "        if f_idx % print_every == 0:\n",
    "            print(f\"{f_idx}\")\n",
    "\n",
    "        results = yolo.track(frame, persist=True, verbose=False, iou=0.6)\n",
    "        r = results[0]\n",
    "        boxes = getattr(r, \"boxes\", None)\n",
    "\n",
    "        if boxes is not None and boxes.id is not None:\n",
    "            ids = boxes.id.cpu().numpy().astype(int)\n",
    "            xyxy = boxes.xyxy.cpu().numpy()  # (N,4)\n",
    "\n",
    "            for tid, bb in zip(ids, xyxy):\n",
    "                x1, y1, x2, y2 = bb.astype(int)\n",
    "                cx = (x1 + x2) // 2\n",
    "                cy = (y1 + y2) // 2\n",
    "                curr = (int(cx), int(cy))\n",
    "                prev = last_center.get(tid, None)\n",
    "\n",
    "                if prev is not None:\n",
    "                    s_prev = _line_side(prev, A, B)\n",
    "                    s_curr = _line_side(curr, A, B)\n",
    "                    crossed = (s_prev != 0 and s_curr != 0 and s_prev != s_curr) or \\\n",
    "                              _segments_intersect(prev, curr, A, B)\n",
    "                    if crossed:\n",
    "                        xx1, yy1 = max(0, x1), max(0, y1)\n",
    "                        xx2, yy2 = min(w, x2), min(h, y2)\n",
    "                        roi = frame[yy1:yy2, xx1:xx2]\n",
    "                        feat = get_features2(roi)      # shape (D,)\n",
    "                        event_frames.append(f_idx)\n",
    "                        event_feats.append(feat.astype(np.float64))\n",
    "\n",
    "                last_center[tid] = curr\n",
    "\n",
    "    cap.release()\n",
    "    total_frames = f_idx + 1\n",
    "    event_frames = np.asarray(event_frames, dtype=int)\n",
    "    event_feats  = np.asarray(event_feats,  dtype=np.float64)  # shape (N,D) or (0,)\n",
    "\n",
    "    # ---- intervals over events for each labeled checkpoint ----\n",
    "    intervals = _build_intervals_from_checkpoints(event_frames, L_frames)\n",
    "\n",
    "    # -------- train (verbose) --------\n",
    "    trainer = GroupedSoftplusSGDVector(\n",
    "        lr=3e-3, epochs=800, l2=1e-5, clip=2.0, report_every=25, early_stop_patience=50\n",
    "    )\n",
    "    trainer.fit(event_feats, intervals, dY, verbose=True)\n",
    "\n",
    "    # raw predictions per interval and cumulative (with alpha)\n",
    "    yhat_evt = trainer.predict_events(event_feats)             # (N,)\n",
    "    inc_pred = []\n",
    "    for a, b in intervals:\n",
    "        inc_pred.append(0.0 if a > b else float(np.sum(yhat_evt[a:b+1])))\n",
    "    inc_pred = np.asarray(inc_pred, dtype=np.float64)\n",
    "    cum_pred = np.cumsum(inc_pred)\n",
    "\n",
    "    # -------- metrics --------\n",
    "    def rmse(x, y): return float(np.sqrt(np.mean((np.asarray(x) - np.asarray(y))**2))) if len(x) else np.nan\n",
    "    def mae(x, y):  return float(np.mean(np.abs(np.asarray(x) - np.asarray(y))))       if len(x) else np.nan\n",
    "\n",
    "    metrics = {\n",
    "        \"events_total\": int(event_feats.shape[0]),\n",
    "        \"feature_dim\": int(event_feats.shape[1]) if event_feats.ndim == 2 and event_feats.size else 0,\n",
    "        \"intervals\": int(len(intervals)),\n",
    "        \"interval_RMSE\": rmse(inc_pred, dY),\n",
    "        \"interval_MAE\":  mae(inc_pred, dY),\n",
    "        \"cumulative_RMSE\": rmse(cum_pred, cum_true),\n",
    "        \"cumulative_MAE\":  mae(cum_pred, cum_true),\n",
    "        \"calibration_alpha\": float(trainer.alpha),\n",
    "        \"empty_intervals_with_positive_truth\":\n",
    "            int(sum(1 for (a,b),dy in zip(intervals, dY) if a > b and dy > 0))\n",
    "    }\n",
    "\n",
    "    print(\"\\n=== weak-supervision evaluation ===\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k:>36}: {v}\")\n",
    "\n",
    "    report = pd.DataFrame({\n",
    "        \"frame_index\": L_frames,\n",
    "        \"inc_true\": dY,\n",
    "        \"inc_pred\": inc_pred,\n",
    "        \"cum_true\": cum_true,\n",
    "        \"cum_pred\": cum_pred\n",
    "    })\n",
    "    print(\"\\ncheckpoint report (head):\\n\", report.head())\n",
    "\n",
    "    # -------- build per-frame overlays and write output video (pass 2) --------\n",
    "    # cum_pred_by_frame: accumulate event outputs up to each frame\n",
    "    cum_pred_by_frame = np.zeros(total_frames, dtype=np.float64)\n",
    "    if yhat_evt.size > 0:\n",
    "        # two-pointer accumulate\n",
    "        ef = event_frames\n",
    "        ev = yhat_evt\n",
    "        j = 0\n",
    "        running = 0.0\n",
    "        for f in range(total_frames):\n",
    "            while j < len(ef) and ef[j] <= f:\n",
    "                running += ev[j]\n",
    "                j += 1\n",
    "            cum_pred_by_frame[f] = running\n",
    "\n",
    "    # cum_true_by_frame: last known label up to frame f\n",
    "    cum_true_by_frame = np.zeros(total_frames, dtype=np.float64)\n",
    "    k = 0\n",
    "    last = 0.0\n",
    "    for f in range(total_frames):\n",
    "        while k < len(L_frames) and L_frames[k] == f:\n",
    "            last = cum_true[k]\n",
    "            k += 1\n",
    "        cum_true_by_frame[f] = last\n",
    "\n",
    "    # write overlay video\n",
    "    if(output_video):\n",
    "        \n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        assert cap.isOpened(), \"Error re-opening video file\"\n",
    "        writer = cv2.VideoWriter(out_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "        f_idx = -1\n",
    "        while True:\n",
    "            ok, frame = cap.read()\n",
    "            if not ok:\n",
    "                break\n",
    "            f_idx += 1\n",
    "\n",
    "            # draw counter line\n",
    "            cv2.line(frame, tuple(line_points[0]), tuple(line_points[1]), (0, 0, 255), 2)\n",
    "\n",
    "            # overlay counts\n",
    "            true_here = cum_true_by_frame[f_idx]\n",
    "            pred_here = cum_pred_by_frame[f_idx]\n",
    "            txt1 = f\"True cum: {true_here:.0f}\"\n",
    "            txt2 = f\"Pred cum: {pred_here:.1f}\"\n",
    "\n",
    "            cv2.putText(frame, txt1, (30, 50), font, 1.0, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "            cv2.putText(frame, txt2, (30, 90), font, 1.0, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "            writer.write(frame)\n",
    "\n",
    "        cap.release()\n",
    "        writer.release()\n",
    "    \n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    return trainer, (event_frames, event_feats), report, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cef099",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer, event_tuple, report, metrics = run_and_train(\n",
    "    #video_path=\"../.local_data/perdue_rgb_video2_061725.mp4\",\n",
    "    #model_path=\"models/best.pt\",\n",
    "    #csv_path=\"../.local_data/counts_xy_true_vs_yolo_series.csv\",\n",
    "    out_path=\"object_counting_output.avi\",\n",
    "    line_points=[(320, 560), (1820, 540)],\n",
    "    draw=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d341bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "true_pred = report.values[:,-2:]\n",
    "\n",
    "plt.plot(true_pred)\n",
    "plt.show()\n",
    "\n",
    "batch_acc = []\n",
    "last_true = 0\n",
    "last_pred = 0\n",
    "b_id = 0\n",
    "\n",
    "for i in range(true_pred.shape[0]):\n",
    "    if((true_pred[i,0] >= (b_id+1)*100) or i==true_pred.shape[0]-1):\n",
    "\n",
    "        batch_acc.append(\n",
    "            (\n",
    "                (true_pred[i,1]-last_pred)\n",
    "                -\n",
    "                (true_pred[i,0]-last_true)\n",
    "            )\n",
    "        )\n",
    "        last_pred = true_pred[i,1]\n",
    "        last_true = true_pred[i,0]\n",
    "        b_id+=1\n",
    "\n",
    "batch_acc = np.asarray(batch_acc, dtype=np.float32)\n",
    "\n",
    "plt.plot(batch_acc)\n",
    "plt.show()\n",
    "\n",
    "print(np.mean(batch_acc))\n",
    "print(np.std(batch_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a7ee37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as pickle\n",
    "with open('tmp_runtrain_data.pkl', 'wb') as file:\n",
    "    pickle.dump([trainer, event_tuple, report, metrics], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ade23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dill as pickle\n",
    "#with open('tmp_runtrain_data.pkl', 'rb') as file:\n",
    "#    tmp_load = pickle.load(file)\n",
    "#    trainer, event_tuple, report, metrics = tmp_load\n",
    "def walk_forward_eval(event_frames, event_feats, L_frames, cum_true, TrainerCtor, verbose=False):\n",
    "    \"\"\"\n",
    "    event_frames: (N,) frame index of each crossing event (sorted)\n",
    "    event_feats:  (N,D) features per event (from your get_features)\n",
    "    L_frames:     (K,) labeled frame indices (sorted)\n",
    "    cum_true:     (K,) cumulative true counts at those frames\n",
    "    TrainerCtor:  callable that returns a *fresh* unfit trainer (e.g., GroupedSoftplusSGDVector)\n",
    "    \"\"\"\n",
    "    # labels as increments\n",
    "    dY = np.diff(np.concatenate([[0.0], cum_true]))\n",
    "    # map checkpoints -> contiguous event-index ranges\n",
    "    intervals = _build_intervals_from_checkpoints(event_frames, L_frames)\n",
    "\n",
    "    K = len(intervals)\n",
    "    inc_pred = np.zeros(K, dtype=float)\n",
    "\n",
    "    for t in range(K):\n",
    "        # training subset = intervals [0 .. t-1]\n",
    "        train_intervals = intervals[:t]\n",
    "        # find last event index included in training (prefix, so contiguous)\n",
    "        last_train_event = max((b for a,b in train_intervals if a <= b), default=-1)\n",
    "        if last_train_event < 0:\n",
    "            # no events yet -> can't learn; predict 0 for first interval\n",
    "            inc_pred[t] = 0.0\n",
    "            continue\n",
    "\n",
    "        X_train = event_feats[:last_train_event+1]\n",
    "        # intervals are already 0..last_train_event, so indices remain valid\n",
    "        dY_train = dY[:t]\n",
    "\n",
    "        trainer = TrainerCtor()\n",
    "        trainer.fit(X_train, train_intervals, dY_train, verbose=False)\n",
    "\n",
    "        # predict interval t using only its events\n",
    "        a, b = intervals[t]\n",
    "        if a > b:\n",
    "            inc_pred[t] = 0.0\n",
    "        else:\n",
    "            y_evt = trainer.predict_events(event_feats[a:b+1])\n",
    "            inc_pred[t] = float(y_evt.sum())\n",
    "\n",
    "        if verbose and (t % max(1, K//10) == 0):\n",
    "            print(f\"[WF] done {t+1}/{K}\")\n",
    "\n",
    "    cum_pred = inc_pred.cumsum()\n",
    "\n",
    "    def rmse(x, y): return float((( (x - y)**2 ).mean())**0.5)\n",
    "    def mae(x, y):  return float(np.abs(x - y).mean())\n",
    "\n",
    "    metrics = {\n",
    "        \"WF_interval_RMSE\": rmse(inc_pred, dY),\n",
    "        \"WF_interval_MAE\":  mae(inc_pred, dY),\n",
    "        \"WF_cumulative_RMSE\": rmse(cum_pred, cum_true),\n",
    "        \"WF_cumulative_MAE\":  mae(cum_pred, cum_true),\n",
    "    }\n",
    "    return inc_pred, cum_pred, metrics\n",
    "\n",
    "\n",
    "inc_pred, cum_pred, metrics = walk_forward_eval(event_tuple[0], event_tuple[1], report['frame_index'].values, report['cum_true'].values, \n",
    "                                                TrainerCtor=lambda: GroupedSoftplusSGDVector(lr=5e-3, epochs=500, l2=1e-2, clip=2.0,\n",
    "                                                 report_every=50, early_stop_patience=50),\n",
    "                                                 verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcb9837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wf_trues = report['inc_true'].values\n",
    "wf_preds = inc_pred\n",
    "\n",
    "errors = np.empty(wf_trues.shape[0], dtype=np.float32)\n",
    "\n",
    "\n",
    "for i in range(true_pred.shape[0]):\n",
    "    \n",
    "    errors[i] = (\n",
    "        (wf_preds[i])\n",
    "        -\n",
    "        (wf_trues[i])\n",
    "    )#))\n",
    "\n",
    "\n",
    "#plt.plot(errors)\n",
    "#plt.title('errors')\n",
    "#plt.show()\n",
    "\n",
    "n= 4\n",
    "\n",
    "err_test    = np.full(errors.shape[0], np.nan, dtype=np.float32)\n",
    "err_test_q  = np.full(errors.shape[0], np.nan, dtype=np.float32)\n",
    "err_test_qma= np.full(errors.shape[0], np.nan, dtype=np.float32)\n",
    "err_test_q99 = np.full((errors.shape[0],n), np.nan, dtype=np.float32)\n",
    "err_test_q50 = np.full((errors.shape[0],n), np.nan, dtype=np.float32)\n",
    "err_test_q10 = np.full((errors.shape[0],n), np.nan, dtype=np.float32)\n",
    "\n",
    "\n",
    "for i in range(err_test.shape[0]):\n",
    "    err_test[i] = abs(errors[i])/(wf_trues[i])\n",
    "\n",
    "qk = 30\n",
    "\n",
    "for i in range(qk, err_test.shape[0]):\n",
    "    err_test_q[i] = np.mean(err_test[i-qk:i])#, q=0.5)\n",
    "\n",
    "k = 30\n",
    "\n",
    "for i in range(qk+k, err_test.shape[0]):\n",
    "    err_test_qma[i] = np.quantile(err_test_q[i-k:i], q=0.99)\n",
    "\n",
    "kn = 6\n",
    "\n",
    "for j in range(n):\n",
    "    k_off = kn*(j+1)\n",
    "    for i in range(k_off, err_test.shape[0]-2):\n",
    "        err_test_q99[i, j] = (np.quantile(\n",
    "            (100/np.sum(wf_trues[i-k_off:i]))*(np.abs(errors[i-k_off:i])), \n",
    "            q=0.99\n",
    "        ))\n",
    "    for i in range(k_off, err_test.shape[0]-2):\n",
    "        err_test_q50[i, j] = (np.quantile(\n",
    "            (100/np.sum(wf_trues[i-k_off:i]))*(np.abs(errors[i-k_off:i])), \n",
    "            q=0.50\n",
    "        ))\n",
    "    for i in range(k_off, err_test.shape[0]-2):\n",
    "        err_test_q10[i, j] = (np.quantile(\n",
    "            (100/np.sum(wf_trues[i-k_off:i]))*(np.abs(errors[i-k_off:i])), \n",
    "            q=0.10\n",
    "        ))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(err_test)\n",
    "plt.title('Miscounts through WF Evaluation\\nWeighted by increment count')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(err_test_q)\n",
    "plt.title(\n",
    "    'Smoothed (30) weighted Miscounts through WF Evaluation')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(err_test_qma)\n",
    "plt.title(\n",
    "    'Worst (Q99, 30) across Smoothed weighted miscounts\\nThrough WF Evaluation')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "err_test_q99[:10,:] = np.nan\n",
    "err_test_q50[:10,:] = np.nan\n",
    "err_test_q10[:10,:] = np.nan\n",
    "\n",
    "#--------------------------\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def maroon_ramp(n_cols,\n",
    "                light_rgb=(245, 225, 225),   # very light maroon-ish\n",
    "                dark_rgb=(215, 135, 135)):   # darker maroon-ish\n",
    "    # Return a list of n_cols colors between light and dark (0–1 floats for mpl)\n",
    "    a = np.array(light_rgb, dtype=float) / 255.0\n",
    "    b = np.array(dark_rgb,  dtype=float) / 255.0\n",
    "    if n_cols == 1:\n",
    "        return [a]\n",
    "    alphas = np.linspace(0, 1, n_cols)\n",
    "    return [tuple(a*(1-t) + b*t) for t in alphas]\n",
    "\n",
    "# Assume qs_clip is shape (T, D) where each column is a series to plot\n",
    "D = err_test_q99.shape[1]\n",
    "colors = maroon_ramp(D, light_rgb=(245,225,225), dark_rgb=(165,35,35))\n",
    "#---------------------------\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_prop_cycle(color=colors)  # set the colors for successive plot lines\n",
    "\n",
    "\n",
    "\n",
    "ax.plot(err_test_q99)                 # this now uses your maroon gradient\n",
    "ax.set_title(\n",
    "    'Worst possible # miscounts expected per batch (of 100)\\nThroughout WF Evaluation')  # your original title\n",
    "#ax.grid(True, alpha=0.2)\n",
    "\n",
    "plt.show()\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_prop_cycle(color=colors)  # set the colors for successive plot lines\n",
    "\n",
    "ax.plot(err_test_q50)                 # this now uses your maroon gradient\n",
    "ax.set_title(\n",
    "    'Most likely # miscounts expected per batch (of 100)\\nThroughout WF Evaluation')  # your original title\n",
    "#ax.grid(True, alpha=0.2)\n",
    "\n",
    "plt.show()\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_prop_cycle(color=colors)  # set the colors for successive plot lines\n",
    "\n",
    "ax.plot(err_test_q10)                 # this now uses your maroon gradient\n",
    "ax.set_title(\n",
    "    'Best 10% # miscounts expected per batch (of 100)\\nThroughout WF Evaluation'\n",
    ")  # your original title\n",
    "#ax.grid(True, alpha=0.2)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9318b8",
   "metadata": {},
   "source": [
    "$i \\in \\text{WF}, \\quad w_i = \\frac{|y_i-\\hat{y}_i|}{y_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c102cd",
   "metadata": {},
   "source": [
    "$k_1=30, i \\in \\{k_1,\\dots, \\text{WF}\\}, s_i = \\frac{1}{k_1}\\sum^{i}_{i-k_1} w_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff85f10",
   "metadata": {},
   "source": [
    "$k_1=k_2=30, i \\in \\{k_1+k_2,\\dots, \\text{WF}\\},$\n",
    "\n",
    "$ Q_{0.99}(\\{s_i : j\\in \\{i-k_2,\\dots, \\text{WF} \\} \\})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d662a5ac",
   "metadata": {},
   "source": [
    "$b \\in \\{1,2,3,4\\}, i \\in \\{6b, \\dots, \\text{WF}\\}$\n",
    "\n",
    "$\\frac{100}{\\sum^{i}_{n=i-6b} y_n} Q_{0.99}(\\{(|y_j - \\hat{y}_j|) : j\\in \\{i-6b,\\dots, i \\} \\})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517aa6e8",
   "metadata": {},
   "source": [
    "$b \\in \\{1,2,3,4\\}, i \\in \\{6b, \\dots, \\text{WF}\\}$\n",
    "\n",
    "$\\frac{100}{\\sum^{i}_{n=i-6b} y_n} \\text{median}(\\{|y_j - \\hat{y}_j| : j\\in \\{i-6b,\\dots, i \\} \\})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fe639e",
   "metadata": {},
   "source": [
    "$b \\in \\{1,2,3,4\\}, i \\in \\{6b, \\dots, \\text{WF}\\}$\n",
    "\n",
    "$\\frac{100}{\\sum^{i}_{n=i-6b} y_n} Q_{0.10}(\\{(|y_j - \\hat{y}_j|) : j\\in \\{i-6b,\\dots, i \\} \\})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9423b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('wf_perf/pkl_cfs01/err_test.pkl', 'wb') as file:\n",
    "    pickle.dump(err_test, file)\n",
    "with open('wf_perf/pkl_cfs01/err_test_q.pkl', 'wb') as file:\n",
    "    pickle.dump(err_test_q, file)\n",
    "with open('wf_perf/pkl_cfs01/err_test_qma.pkl', 'wb') as file:\n",
    "    pickle.dump(err_test_qma, file)\n",
    "with open('wf_perf/pkl_cfs01/err_test_q99.pkl', 'wb') as file:\n",
    "    pickle.dump(err_test_q99, file)\n",
    "with open('wf_perf/pkl_cfs01/err_test_q50.pkl', 'wb') as file:\n",
    "    pickle.dump(err_test_q50, file)\n",
    "with open('wf_perf/pkl_cfs01/err_test_q10.pkl', 'wb') as file:\n",
    "    pickle.dump(err_test_q10, file)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bda254",
   "metadata": {},
   "source": [
    "## *Comparing Performances*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589cd5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- Load ----------\n",
    "with open('wf_perf/pkl_init/err_test.pkl', 'rb') as file:\n",
    "    init_w = pickle.load(file)\n",
    "with open('wf_perf/pkl_JJy/err_test.pkl', 'rb') as file:\n",
    "    yfJJ_w = pickle.load(file)\n",
    "with open('wf_perf/pkl_fccat/err_test.pkl', 'rb') as file:\n",
    "    fcat_w = pickle.load(file)\n",
    "\n",
    "with open('wf_perf/pkl_init/err_test_q.pkl', 'rb') as file:\n",
    "    init_q = pickle.load(file)\n",
    "with open('wf_perf/pkl_JJy/err_test_q.pkl', 'rb') as file:\n",
    "    yfJJ_q = pickle.load(file)\n",
    "with open('wf_perf/pkl_fccat/err_test_q.pkl', 'rb') as file:\n",
    "    fcat_q = pickle.load(file)\n",
    "\n",
    "with open('wf_perf/pkl_init/err_test_qma.pkl', 'rb') as file:\n",
    "    init_qma = pickle.load(file)\n",
    "with open('wf_perf/pkl_JJy/err_test_qma.pkl', 'rb') as file:\n",
    "    yfJJ_qma = pickle.load(file)\n",
    "with open('wf_perf/pkl_fccat/err_test_qma.pkl', 'rb') as file:\n",
    "    fcat_qma = pickle.load(file)\n",
    "\n",
    "with open('wf_perf/pkl_init/err_test_q99.pkl', 'rb') as file:\n",
    "    init_q99 = pickle.load(file)\n",
    "with open('wf_perf/pkl_JJy/err_test_q99.pkl', 'rb') as file:\n",
    "    yfJJ_q99 = pickle.load(file)\n",
    "with open('wf_perf/pkl_fccat/err_test_q99.pkl', 'rb') as file:\n",
    "    fcat_q99 = pickle.load(file)\n",
    "\n",
    "with open('wf_perf/pkl_init/err_test_q50.pkl', 'rb') as file:\n",
    "    init_q50 = pickle.load(file)\n",
    "with open('wf_perf/pkl_JJy/err_test_q50.pkl', 'rb') as file:\n",
    "    yfJJ_q50 = pickle.load(file)\n",
    "with open('wf_perf/pkl_fccat/err_test_q50.pkl', 'rb') as file:\n",
    "    fcat_q50 = pickle.load(file)\n",
    "\n",
    "with open('wf_perf/pkl_init/err_test_q10.pkl', 'rb') as file:\n",
    "    init_q10 = pickle.load(file)\n",
    "with open('wf_perf/pkl_JJy/err_test_q10.pkl', 'rb') as file:\n",
    "    yfJJ_q10 = pickle.load(file)\n",
    "with open('wf_perf/pkl_fccat/err_test_q10.pkl', 'rb') as file:\n",
    "    fcat_q10 = pickle.load(file)\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def plot_trio(ax, a, b, c, title):\n",
    "    ax.plot(a, label='Init')\n",
    "    ax.plot(b, label='yfJJ')\n",
    "    ax.plot(c, label='fcat')\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "\n",
    "def plot_batches_1x2(init_arr, yfJJ_arr, fcat_arr, qlabel, figsize=(12, 4)):\n",
    "    \"\"\"Render two batches per figure: left = batch i, right = batch i+1.\"\"\"\n",
    "    num_batches = init_arr.shape[1]\n",
    "    for i in range(0, num_batches, 2):\n",
    "        fig, ax = plt.subplots(1, 2, figsize=figsize, sharey=True)\n",
    "\n",
    "        # Left: batch i\n",
    "        plot_trio(ax[0],\n",
    "                  init_arr[:, i], yfJJ_arr[:, i], fcat_arr[:, i],\n",
    "                  f'{qlabel} on {i+1} batches')\n",
    "\n",
    "        # Right: batch i+1 if available\n",
    "        if i + 1 < num_batches:\n",
    "            plot_trio(ax[1],\n",
    "                      init_arr[:, i+1], yfJJ_arr[:, i+1], fcat_arr[:, i+1],\n",
    "                      f'{qlabel} on {i+2} batches')\n",
    "        else:\n",
    "            ax[1].axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# ---------- Top-level summaries (unchanged, single figures) ----------\n",
    "plt.plot(init_w); plt.plot(yfJJ_w); plt.plot(fcat_w)\n",
    "plt.title('err_test (Init vs yfJJ vs fcat)')\n",
    "plt.legend(['Init','yfJJ','fcat'])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(init_q); plt.plot(yfJJ_q); plt.plot(fcat_q)\n",
    "plt.title('err_test_q (Init vs yfJJ vs fcat)')\n",
    "plt.legend(['Init','yfJJ','fcat'])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(init_qma); plt.plot(yfJJ_qma); plt.plot(fcat_qma)\n",
    "plt.title('err_test_qma (Init vs yfJJ vs fcat)')\n",
    "plt.legend(['Init','yfJJ','fcat'])\n",
    "plt.show()\n",
    "\n",
    "# ---------- Per-batch loops now 1x2 each ----------\n",
    "plot_batches_1x2(init_q99, yfJJ_q99, fcat_q99, 'Q99')\n",
    "plot_batches_1x2(init_q50, yfJJ_q50, fcat_q50, 'Q50')\n",
    "plot_batches_1x2(init_q10, yfJJ_q10, fcat_q10, 'Q10')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891a3bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install bottleneck"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0043778b",
   "metadata": {},
   "source": [
    "### Combined fspace Loss - Yellow fspace Loss\n",
    "### Combined fspace Loss - Initial fspace Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff2c0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bottleneck as bn\n",
    "\n",
    "#weighted error at each increment\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4), sharey=True)\n",
    "ax[0].plot(bn.move_mean(fcat_w-init_w, 5), c=(0.25,0.25,1))\n",
    "ax[0].plot(bn.move_mean(fcat_w-init_w, 25), c=(0.5,0.5,1))\n",
    "ax[0].set_title('Weighted loss:\\nCombined against Initial features')\n",
    "ax[0].axhline(0, c='black', alpha=0.5)\n",
    "\n",
    "ax[1].plot(bn.move_mean(fcat_w-yfJJ_w, 5), c=(1,0.25,0.25))\n",
    "ax[1].plot(bn.move_mean(fcat_w-yfJJ_w, 25), c=(1,0.5,0.5))\n",
    "ax[1].set_title('Weighted loss:\\nCombined against Yellow features')\n",
    "ax[1].axhline(0, c='black', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# median performances (side-by-side per batch)\n",
    "for i in range(4):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 4), sharey=True)\n",
    "\n",
    "    # Left: vs Initial features\n",
    "    ax[0].plot(bn.move_mean(fcat_q50[:, i] - init_q50[:, i], 10), c=(0.25, 0.25, 1))\n",
    "    ax[0].plot(bn.move_mean(fcat_q50[:, i] - init_q50[:, i], 25), c=(0.5, 0.5, 1))\n",
    "    ax[0].axhline(0, c='black', alpha=0.5)\n",
    "    ax[0].set_title(f'Median estimated error in {i+1} Batch(es):\\nComparison with combined and Initial features')\n",
    "\n",
    "    # Right: vs Yellow features\n",
    "    ax[1].plot(bn.move_mean(fcat_q50[:, i] - yfJJ_q50[:, i], 10), c=(1, 0.25, 0.25))\n",
    "    ax[1].plot(bn.move_mean(fcat_q50[:, i] - yfJJ_q50[:, i], 25), c=(1, 0.5, 0.5))\n",
    "    ax[1].axhline(0, c='black', alpha=0.5)\n",
    "    ax[1].set_title(f'Median estimated error in {i+1} Batch(es):\\nComparison with combined and Yellow features')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
